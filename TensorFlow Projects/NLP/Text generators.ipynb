{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "pad_sequences = tf.keras.preprocessing.sequence.pad_sequences\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/songs/IrishSong.txt\", \"r\") as file:\n",
    "    IrishSong = file.read().splitlines()\n",
    "IrishSong = list(map(str.strip, IrishSong))\n",
    "IrishSong = list(map(str.lower, IrishSong))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(tokenizer, IrishSong):\n",
    "    tokenizer.fit_on_texts(IrishSong)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    input_sequences = []\n",
    "    for line in IrishSong:\n",
    "        tokenised_line = tokenizer.texts_to_sequences([line])[0] # token_list.shape = (1,len(line))\n",
    "        for i in range(1,len(tokenised_line)):\n",
    "            input_sequences.append(tokenised_line[:i+1])\n",
    "\n",
    "    max_sequence_len = max(len(seq) for seq in input_sequences)\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding=\"pre\"))\n",
    "\n",
    "    xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    ys = tf.keras.utils.to_categorical(labels, num_classes=total_words) # onehot encoding of labels\n",
    "    \n",
    "    return xs, ys, max_sequence_len, total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(IrishSong)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "input_sequences = []\n",
    "for line in IrishSong:\n",
    "    tokenised_line = tokenizer.texts_to_sequences([line])[0] # token_list.shape = (1,len(line))\n",
    "    for i in range(1,len(tokenised_line)):\n",
    "        input_sequences.append(tokenised_line[:i+1])\n",
    "\n",
    "max_sequence_len = max(len(seq) for seq in input_sequences)\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding=\"pre\"))\n",
    "\n",
    "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words) # onehot encoding of labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys, max_sequence_len, total_words = preprocessor(tokenizer, IrishSong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(total_words, 64, input_length=max_sequence_len-1),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20)),\n",
    "    tf.keras.layers.Dense(total_words, activation=\"softmax\")\n",
    "])\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"data/model-checkpoints/text-Generation/\", save_weights_only=True, verbose=1)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 453 samples\n",
      "Epoch 1/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 5.5683 - accuracy: 0.0112\n",
      "Epoch 00001: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 3s 8ms/sample - loss: 5.5684 - accuracy: 0.0132\n",
      "Epoch 2/500\n",
      "416/453 [==========================>...] - ETA: 0s - loss: 5.5435 - accuracy: 0.0361\n",
      "Epoch 00002: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 573us/sample - loss: 5.5432 - accuracy: 0.0331\n",
      "Epoch 3/500\n",
      "416/453 [==========================>...] - ETA: 0s - loss: 5.4796 - accuracy: 0.0457\n",
      "Epoch 00003: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 561us/sample - loss: 5.4796 - accuracy: 0.0464\n",
      "Epoch 4/500\n",
      "416/453 [==========================>...] - ETA: 0s - loss: 5.3201 - accuracy: 0.0553\n",
      "Epoch 00004: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 430us/sample - loss: 5.2850 - accuracy: 0.0530\n",
      "Epoch 5/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 5.0855 - accuracy: 0.0540\n",
      "Epoch 00005: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 463us/sample - loss: 5.1136 - accuracy: 0.0508\n",
      "Epoch 6/500\n",
      "384/453 [========================>.....] - ETA: 0s - loss: 5.0325 - accuracy: 0.0521\n",
      "Epoch 00006: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 435us/sample - loss: 5.0578 - accuracy: 0.0508\n",
      "Epoch 7/500\n",
      "384/453 [========================>.....] - ETA: 0s - loss: 5.0277 - accuracy: 0.0547\n",
      "Epoch 00007: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 468us/sample - loss: 5.0229 - accuracy: 0.0508\n",
      "Epoch 8/500\n",
      "384/453 [========================>.....] - ETA: 0s - loss: 4.9367 - accuracy: 0.0625\n",
      "Epoch 00008: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 422us/sample - loss: 4.9948 - accuracy: 0.0596\n",
      "Epoch 9/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 4.9116 - accuracy: 0.0597\n",
      "Epoch 00009: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 457us/sample - loss: 4.9604 - accuracy: 0.0618\n",
      "Epoch 10/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 4.9556 - accuracy: 0.0483  \n",
      "Epoch 00010: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 539us/sample - loss: 4.9224 - accuracy: 0.0662\n",
      "Epoch 11/500\n",
      "416/453 [==========================>...] - ETA: 0s - loss: 4.8714 - accuracy: 0.0625\n",
      "Epoch 00011: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 545us/sample - loss: 4.8793 - accuracy: 0.0596\n",
      "Epoch 12/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 4.8346 - accuracy: 0.0483\n",
      "Epoch 00012: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 510us/sample - loss: 4.8368 - accuracy: 0.0552\n",
      "Epoch 13/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 4.7591 - accuracy: 0.0852\n",
      "Epoch 00013: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 4.7917 - accuracy: 0.0795\n",
      "Epoch 14/500\n",
      "416/453 [==========================>...] - ETA: 0s - loss: 4.7624 - accuracy: 0.0625\n",
      "Epoch 00014: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 382us/sample - loss: 4.7419 - accuracy: 0.0706\n",
      "Epoch 15/500\n",
      "384/453 [========================>.....] - ETA: 0s - loss: 4.6704 - accuracy: 0.0885\n",
      "Epoch 00015: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 424us/sample - loss: 4.6853 - accuracy: 0.0861\n",
      "Epoch 16/500\n",
      "416/453 [==========================>...] - ETA: 0s - loss: 4.6390 - accuracy: 0.0841\n",
      "Epoch 00016: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 391us/sample - loss: 4.6357 - accuracy: 0.0861\n",
      "Epoch 17/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 4.5512 - accuracy: 0.0909\n",
      "Epoch 00017: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 444us/sample - loss: 4.5881 - accuracy: 0.0795\n",
      "Epoch 18/500\n",
      "384/453 [========================>.....] - ETA: 0s - loss: 4.4795 - accuracy: 0.1068\n",
      "Epoch 00018: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 411us/sample - loss: 4.5391 - accuracy: 0.1015\n",
      "Epoch 19/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 4.5041 - accuracy: 0.0824\n",
      "Epoch 00019: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 528us/sample - loss: 4.4827 - accuracy: 0.0927\n",
      "Epoch 20/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 4.4270 - accuracy: 0.0852\n",
      "Epoch 00020: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 426us/sample - loss: 4.4553 - accuracy: 0.0861\n",
      "Epoch 21/500\n",
      "384/453 [========================>.....] - ETA: 0s - loss: 4.3901 - accuracy: 0.1120\n",
      "Epoch 00021: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 468us/sample - loss: 4.4002 - accuracy: 0.1104\n",
      "Epoch 22/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 4.3663 - accuracy: 0.0906\n",
      "Epoch 00022: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 461us/sample - loss: 4.3562 - accuracy: 0.1060\n",
      "Epoch 23/500\n",
      "416/453 [==========================>...] - ETA: 0s - loss: 4.3323 - accuracy: 0.0986\n",
      "Epoch 00023: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 444us/sample - loss: 4.3085 - accuracy: 0.1060\n",
      "Epoch 24/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 4.2675 - accuracy: 0.1138\n",
      "Epoch 00024: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 534us/sample - loss: 4.2622 - accuracy: 0.1126\n",
      "Epoch 25/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 4.3184 - accuracy: 0.1094\n",
      "Epoch 00025: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 486us/sample - loss: 4.2281 - accuracy: 0.1148\n",
      "Epoch 26/500\n",
      "384/453 [========================>.....] - ETA: 0s - loss: 4.2148 - accuracy: 0.1224\n",
      "Epoch 00026: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 4.1776 - accuracy: 0.1302\n",
      "Epoch 27/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 4.1657 - accuracy: 0.1335\n",
      "Epoch 00027: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 397us/sample - loss: 4.1262 - accuracy: 0.1369\n",
      "Epoch 28/500\n",
      "416/453 [==========================>...] - ETA: 0s - loss: 4.0984 - accuracy: 0.1346\n",
      "Epoch 00028: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 400us/sample - loss: 4.0879 - accuracy: 0.1391\n",
      "Epoch 29/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 4.0232 - accuracy: 0.1534\n",
      "Epoch 00029: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 446us/sample - loss: 4.0428 - accuracy: 0.1435\n",
      "Epoch 30/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 4.0145 - accuracy: 0.1449\n",
      "Epoch 00030: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 464us/sample - loss: 4.0044 - accuracy: 0.1435\n",
      "Epoch 31/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 3.9344 - accuracy: 0.1648\n",
      "Epoch 00031: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 453us/sample - loss: 3.9650 - accuracy: 0.1523\n",
      "Epoch 32/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 3.9196 - accuracy: 0.1585\n",
      "Epoch 00032: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 356us/sample - loss: 3.9233 - accuracy: 0.1589\n",
      "Epoch 33/500\n",
      "416/453 [==========================>...] - ETA: 0s - loss: 3.9115 - accuracy: 0.1755\n",
      "Epoch 00033: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 422us/sample - loss: 3.8888 - accuracy: 0.1832\n",
      "Epoch 34/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 3.8565 - accuracy: 0.1818\n",
      "Epoch 00034: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 459us/sample - loss: 3.8425 - accuracy: 0.1700\n",
      "Epoch 35/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 3.8007 - accuracy: 0.1763\n",
      "Epoch 00035: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 497us/sample - loss: 3.7998 - accuracy: 0.1766\n",
      "Epoch 36/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 3.8102 - accuracy: 0.1705\n",
      "Epoch 00036: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 508us/sample - loss: 3.7686 - accuracy: 0.1854\n",
      "Epoch 37/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 3.7280 - accuracy: 0.2017\n",
      "Epoch 00037: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 3.7348 - accuracy: 0.2031\n",
      "Epoch 38/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 3.7001 - accuracy: 0.2074\n",
      "Epoch 00038: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 475us/sample - loss: 3.6867 - accuracy: 0.2075\n",
      "Epoch 39/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 3.6098 - accuracy: 0.2244\n",
      "Epoch 00039: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 428us/sample - loss: 3.6557 - accuracy: 0.2141\n",
      "Epoch 40/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 3.6419 - accuracy: 0.2017\n",
      "Epoch 00040: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 437us/sample - loss: 3.6176 - accuracy: 0.2141\n",
      "Epoch 41/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 3.6031 - accuracy: 0.2273\n",
      "Epoch 00041: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 437us/sample - loss: 3.5856 - accuracy: 0.2230\n",
      "Epoch 42/500\n",
      "384/453 [========================>.....] - ETA: 0s - loss: 3.5051 - accuracy: 0.2214\n",
      "Epoch 00042: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 455us/sample - loss: 3.5616 - accuracy: 0.2185\n",
      "Epoch 43/500\n",
      "384/453 [========================>.....] - ETA: 0s - loss: 3.5081 - accuracy: 0.2474\n",
      "Epoch 00043: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 435us/sample - loss: 3.5160 - accuracy: 0.2428\n",
      "Epoch 44/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 3.4651 - accuracy: 0.2531\n",
      "Epoch 00044: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 479us/sample - loss: 3.4826 - accuracy: 0.2472\n",
      "Epoch 45/500\n",
      "416/453 [==========================>...] - ETA: 0s - loss: 3.4575 - accuracy: 0.2476\n",
      "Epoch 00045: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 415us/sample - loss: 3.4446 - accuracy: 0.2517\n",
      "Epoch 46/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 3.4400 - accuracy: 0.2415\n",
      "Epoch 00046: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 435us/sample - loss: 3.4124 - accuracy: 0.2494\n",
      "Epoch 47/500\n",
      "384/453 [========================>.....] - ETA: 0s - loss: 3.3939 - accuracy: 0.2604\n",
      "Epoch 00047: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 426us/sample - loss: 3.3856 - accuracy: 0.2605\n",
      "Epoch 48/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 3.3726 - accuracy: 0.2585\n",
      "Epoch 00048: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 433us/sample - loss: 3.3674 - accuracy: 0.2627\n",
      "Epoch 49/500\n",
      "384/453 [========================>.....] - ETA: 0s - loss: 3.3257 - accuracy: 0.2865\n",
      "Epoch 00049: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 411us/sample - loss: 3.3190 - accuracy: 0.2848\n",
      "Epoch 50/500\n",
      "384/453 [========================>.....] - ETA: 0s - loss: 3.2707 - accuracy: 0.2995\n",
      "Epoch 00050: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 435us/sample - loss: 3.2782 - accuracy: 0.3046\n",
      "Epoch 51/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 3.2518 - accuracy: 0.2898\n",
      "Epoch 00051: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 466us/sample - loss: 3.2370 - accuracy: 0.3179\n",
      "Epoch 52/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 3.1985 - accuracy: 0.3214\n",
      "Epoch 00052: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 517us/sample - loss: 3.1992 - accuracy: 0.3223\n",
      "Epoch 53/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 3.1670 - accuracy: 0.3324\n",
      "Epoch 00053: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 437us/sample - loss: 3.1617 - accuracy: 0.3355\n",
      "Epoch 54/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 3.1298 - accuracy: 0.3504\n",
      "Epoch 00054: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 546us/sample - loss: 3.1292 - accuracy: 0.3510\n",
      "Epoch 55/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 3.1142 - accuracy: 0.3504\n",
      "Epoch 00055: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 521us/sample - loss: 3.1072 - accuracy: 0.3532\n",
      "Epoch 56/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 3.0952 - accuracy: 0.3594\n",
      "Epoch 00056: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 494us/sample - loss: 3.0703 - accuracy: 0.3687\n",
      "Epoch 57/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 3.0329 - accuracy: 0.3636\n",
      "Epoch 00057: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 466us/sample - loss: 3.0413 - accuracy: 0.3709\n",
      "Epoch 58/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 3.0031 - accuracy: 0.4148\n",
      "Epoch 00058: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 457us/sample - loss: 3.0126 - accuracy: 0.3929\n",
      "Epoch 59/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 2.9650 - accuracy: 0.3977\n",
      "Epoch 00059: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 450us/sample - loss: 2.9803 - accuracy: 0.3863\n",
      "Epoch 60/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 2.9831 - accuracy: 0.3906\n",
      "Epoch 00060: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 477us/sample - loss: 2.9563 - accuracy: 0.4106\n",
      "Epoch 61/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 2.9388 - accuracy: 0.4176\n",
      "Epoch 00061: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 483us/sample - loss: 2.9316 - accuracy: 0.4238\n",
      "Epoch 62/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 2.9162 - accuracy: 0.4187\n",
      "Epoch 00062: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 466us/sample - loss: 2.9038 - accuracy: 0.4216\n",
      "Epoch 63/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 2.8412 - accuracy: 0.4375\n",
      "Epoch 00063: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 464us/sample - loss: 2.8725 - accuracy: 0.4172\n",
      "Epoch 64/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 2.8953 - accuracy: 0.4233\n",
      "Epoch 00064: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 459us/sample - loss: 2.8598 - accuracy: 0.4238\n",
      "Epoch 65/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 2.8800 - accuracy: 0.4174\n",
      "Epoch 00065: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 499us/sample - loss: 2.8808 - accuracy: 0.4172\n",
      "Epoch 66/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 2.8514 - accuracy: 0.4233\n",
      "Epoch 00066: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 457us/sample - loss: 2.8448 - accuracy: 0.4283\n",
      "Epoch 67/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 2.7529 - accuracy: 0.4545\n",
      "Epoch 00067: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 460us/sample - loss: 2.7868 - accuracy: 0.4503\n",
      "Epoch 68/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 2.8199 - accuracy: 0.4594\n",
      "Epoch 00068: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 488us/sample - loss: 2.7448 - accuracy: 0.4768\n",
      "Epoch 69/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 2.7170 - accuracy: 0.4773\n",
      "Epoch 00069: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 453us/sample - loss: 2.7165 - accuracy: 0.4636\n",
      "Epoch 70/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 2.6912 - accuracy: 0.5028\n",
      "Epoch 00070: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 457us/sample - loss: 2.6917 - accuracy: 0.5011\n",
      "Epoch 71/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 2.6488 - accuracy: 0.5170\n",
      "Epoch 00071: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 459us/sample - loss: 2.6529 - accuracy: 0.5210\n",
      "Epoch 72/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 2.6350 - accuracy: 0.5256\n",
      "Epoch 00072: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 554us/sample - loss: 2.6300 - accuracy: 0.5210\n",
      "Epoch 73/500\n",
      "416/453 [==========================>...] - ETA: 0s - loss: 2.5792 - accuracy: 0.5288\n",
      "Epoch 00073: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 532us/sample - loss: 2.6035 - accuracy: 0.5210\n",
      "Epoch 74/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 2.5937 - accuracy: 0.5250\n",
      "Epoch 00074: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 468us/sample - loss: 2.5798 - accuracy: 0.5320\n",
      "Epoch 75/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 2.4876 - accuracy: 0.5625\n",
      "Epoch 00075: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 483us/sample - loss: 2.5567 - accuracy: 0.5320\n",
      "Epoch 76/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 2.5228 - accuracy: 0.5455\n",
      "Epoch 00076: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 466us/sample - loss: 2.5331 - accuracy: 0.5342\n",
      "Epoch 77/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 2.4354 - accuracy: 0.5710\n",
      "Epoch 00077: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 446us/sample - loss: 2.5086 - accuracy: 0.5408\n",
      "Epoch 78/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 2.4582 - accuracy: 0.5375\n",
      "Epoch 00078: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 486us/sample - loss: 2.4833 - accuracy: 0.5386\n",
      "Epoch 79/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 2.4162 - accuracy: 0.5719\n",
      "Epoch 00079: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 461us/sample - loss: 2.4493 - accuracy: 0.5563\n",
      "Epoch 80/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 2.3921 - accuracy: 0.5881\n",
      "Epoch 00080: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 450us/sample - loss: 2.4251 - accuracy: 0.5717\n",
      "Epoch 81/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 2.3694 - accuracy: 0.6000\n",
      "Epoch 00081: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 528us/sample - loss: 2.4004 - accuracy: 0.5850\n",
      "Epoch 82/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 2.3750 - accuracy: 0.5881\n",
      "Epoch 00082: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 439us/sample - loss: 2.3796 - accuracy: 0.5850\n",
      "Epoch 83/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 2.3386 - accuracy: 0.6051\n",
      "Epoch 00083: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 439us/sample - loss: 2.3562 - accuracy: 0.5828\n",
      "Epoch 84/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 2.3347 - accuracy: 0.5824\n",
      "Epoch 00084: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 459us/sample - loss: 2.3448 - accuracy: 0.5894\n",
      "Epoch 85/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 2.3701 - accuracy: 0.5795\n",
      "Epoch 00085: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 446us/sample - loss: 2.3363 - accuracy: 0.5982\n",
      "Epoch 86/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 2.3158 - accuracy: 0.5938\n",
      "Epoch 00086: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 446us/sample - loss: 2.3077 - accuracy: 0.5982\n",
      "Epoch 87/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 2.2600 - accuracy: 0.6051\n",
      "Epoch 00087: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 453us/sample - loss: 2.2793 - accuracy: 0.6049\n",
      "Epoch 88/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 2.2479 - accuracy: 0.5966\n",
      "Epoch 00088: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 450us/sample - loss: 2.2555 - accuracy: 0.6026\n",
      "Epoch 89/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 2.2128 - accuracy: 0.6364\n",
      "Epoch 00089: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 453us/sample - loss: 2.2288 - accuracy: 0.6313\n",
      "Epoch 90/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 2.2070 - accuracy: 0.6307\n",
      "Epoch 00090: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 452us/sample - loss: 2.2098 - accuracy: 0.6336\n",
      "Epoch 91/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 2.1668 - accuracy: 0.6165\n",
      "Epoch 00091: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 459us/sample - loss: 2.1846 - accuracy: 0.6137\n",
      "Epoch 92/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 2.1568 - accuracy: 0.6222\n",
      "Epoch 00092: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 488us/sample - loss: 2.1586 - accuracy: 0.6247\n",
      "Epoch 93/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 2.2282 - accuracy: 0.6031\n",
      "Epoch 00093: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 479us/sample - loss: 2.1542 - accuracy: 0.6313\n",
      "Epoch 94/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 2.1194 - accuracy: 0.6676\n",
      "Epoch 00094: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 472us/sample - loss: 2.1293 - accuracy: 0.6512\n",
      "Epoch 95/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 2.1174 - accuracy: 0.6392\n",
      "Epoch 00095: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 477us/sample - loss: 2.1221 - accuracy: 0.6446\n",
      "Epoch 96/500\n",
      "384/453 [========================>.....] - ETA: 0s - loss: 2.0738 - accuracy: 0.6641\n",
      "Epoch 00096: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 439us/sample - loss: 2.0928 - accuracy: 0.6578\n",
      "Epoch 97/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 2.0337 - accuracy: 0.6733\n",
      "Epoch 00097: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 459us/sample - loss: 2.0650 - accuracy: 0.6623\n",
      "Epoch 98/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 2.0424 - accuracy: 0.6648\n",
      "Epoch 00098: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 452us/sample - loss: 2.0406 - accuracy: 0.6600\n",
      "Epoch 99/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 2.0197 - accuracy: 0.6674\n",
      "Epoch 00099: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 512us/sample - loss: 2.0373 - accuracy: 0.6645\n",
      "Epoch 100/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 1.9636 - accuracy: 0.6750\n",
      "Epoch 00100: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 466us/sample - loss: 2.0150 - accuracy: 0.6667\n",
      "Epoch 101/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.9854 - accuracy: 0.6648\n",
      "Epoch 00101: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 459us/sample - loss: 1.9969 - accuracy: 0.6711\n",
      "Epoch 102/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.9797 - accuracy: 0.6562\n",
      "Epoch 00102: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 450us/sample - loss: 1.9724 - accuracy: 0.6645\n",
      "Epoch 103/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.9757 - accuracy: 0.6562\n",
      "Epoch 00103: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 453us/sample - loss: 1.9804 - accuracy: 0.6578\n",
      "Epoch 104/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.9105 - accuracy: 0.6534\n",
      "Epoch 00104: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 439us/sample - loss: 1.9610 - accuracy: 0.6578\n",
      "Epoch 105/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.9040 - accuracy: 0.6818\n",
      "Epoch 00105: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 453us/sample - loss: 1.9257 - accuracy: 0.6623\n",
      "Epoch 106/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.9721 - accuracy: 0.6335\n",
      "Epoch 00106: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 1.9993 - accuracy: 0.6358\n",
      "Epoch 107/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.9955 - accuracy: 0.6307\n",
      "Epoch 00107: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 444us/sample - loss: 1.9656 - accuracy: 0.6358\n",
      "Epoch 108/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.9388 - accuracy: 0.6705\n",
      "Epoch 00108: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 450us/sample - loss: 1.9275 - accuracy: 0.6623\n",
      "Epoch 109/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.9247 - accuracy: 0.6307\n",
      "Epoch 00109: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 450us/sample - loss: 1.9157 - accuracy: 0.6446\n",
      "Epoch 110/500\n",
      "384/453 [========================>.....] - ETA: 0s - loss: 1.8703 - accuracy: 0.6641\n",
      "Epoch 00110: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 444us/sample - loss: 1.8622 - accuracy: 0.6667\n",
      "Epoch 111/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 1.8240 - accuracy: 0.6844\n",
      "Epoch 00111: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 472us/sample - loss: 1.8367 - accuracy: 0.6623\n",
      "Epoch 112/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.8028 - accuracy: 0.6960\n",
      "Epoch 00112: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 450us/sample - loss: 1.8113 - accuracy: 0.6755\n",
      "Epoch 113/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.7692 - accuracy: 0.6903\n",
      "Epoch 00113: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 446us/sample - loss: 1.7810 - accuracy: 0.6821\n",
      "Epoch 114/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.7394 - accuracy: 0.7102\n",
      "Epoch 00114: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 444us/sample - loss: 1.7627 - accuracy: 0.6887\n",
      "Epoch 115/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.7411 - accuracy: 0.7159\n",
      "Epoch 00115: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 458us/sample - loss: 1.7421 - accuracy: 0.6976\n",
      "Epoch 116/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.7370 - accuracy: 0.6818\n",
      "Epoch 00116: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 1.7221 - accuracy: 0.6932\n",
      "Epoch 117/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.7210 - accuracy: 0.6903\n",
      "Epoch 00117: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 455us/sample - loss: 1.7025 - accuracy: 0.6954\n",
      "Epoch 118/500\n",
      "384/453 [========================>.....] - ETA: 0s - loss: 1.6999 - accuracy: 0.7005\n",
      "Epoch 00118: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 442us/sample - loss: 1.6853 - accuracy: 0.7086\n",
      "Epoch 119/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.6781 - accuracy: 0.7045\n",
      "Epoch 00119: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 455us/sample - loss: 1.6668 - accuracy: 0.7042\n",
      "Epoch 120/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.6488 - accuracy: 0.7074\n",
      "Epoch 00120: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 433us/sample - loss: 1.6484 - accuracy: 0.7086\n",
      "Epoch 121/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.6584 - accuracy: 0.7017\n",
      "Epoch 00121: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 439us/sample - loss: 1.6349 - accuracy: 0.7263\n",
      "Epoch 122/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 1.6253 - accuracy: 0.7281\n",
      "Epoch 00122: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 470us/sample - loss: 1.6165 - accuracy: 0.7263\n",
      "Epoch 123/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.5800 - accuracy: 0.7358\n",
      "Epoch 00123: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 441us/sample - loss: 1.6026 - accuracy: 0.7307\n",
      "Epoch 124/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.5269 - accuracy: 0.7415\n",
      "Epoch 00124: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 461us/sample - loss: 1.5870 - accuracy: 0.7307\n",
      "Epoch 125/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.5537 - accuracy: 0.7330\n",
      "Epoch 00125: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 444us/sample - loss: 1.5730 - accuracy: 0.7263\n",
      "Epoch 126/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.5439 - accuracy: 0.7273\n",
      "Epoch 00126: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 446us/sample - loss: 1.5545 - accuracy: 0.7285\n",
      "Epoch 127/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.5328 - accuracy: 0.7301\n",
      "Epoch 00127: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 459us/sample - loss: 1.5387 - accuracy: 0.7285\n",
      "Epoch 128/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.5532 - accuracy: 0.7131\n",
      "Epoch 00128: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 494us/sample - loss: 1.5278 - accuracy: 0.7285\n",
      "Epoch 129/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.4889 - accuracy: 0.7528\n",
      "Epoch 00129: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 450us/sample - loss: 1.5111 - accuracy: 0.7329\n",
      "Epoch 130/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.4462 - accuracy: 0.7557\n",
      "Epoch 00130: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 451us/sample - loss: 1.5024 - accuracy: 0.7461\n",
      "Epoch 131/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.4943 - accuracy: 0.7472\n",
      "Epoch 00131: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 466us/sample - loss: 1.4883 - accuracy: 0.7439\n",
      "Epoch 132/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 1.4837 - accuracy: 0.7469\n",
      "Epoch 00132: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 475us/sample - loss: 1.4717 - accuracy: 0.7439\n",
      "Epoch 133/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.4700 - accuracy: 0.7358\n",
      "Epoch 00133: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 446us/sample - loss: 1.4785 - accuracy: 0.7373\n",
      "Epoch 134/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.4703 - accuracy: 0.7358\n",
      "Epoch 00134: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 450us/sample - loss: 1.4583 - accuracy: 0.7417\n",
      "Epoch 135/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.4251 - accuracy: 0.7585\n",
      "Epoch 00135: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 455us/sample - loss: 1.4511 - accuracy: 0.7483\n",
      "Epoch 136/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.4432 - accuracy: 0.7472\n",
      "Epoch 00136: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 433us/sample - loss: 1.4278 - accuracy: 0.7506\n",
      "Epoch 137/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.4305 - accuracy: 0.7557\n",
      "Epoch 00137: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 459us/sample - loss: 1.4116 - accuracy: 0.7594\n",
      "Epoch 138/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.3721 - accuracy: 0.7812\n",
      "Epoch 00138: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 444us/sample - loss: 1.3904 - accuracy: 0.7616\n",
      "Epoch 139/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.3963 - accuracy: 0.7528\n",
      "Epoch 00139: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 444us/sample - loss: 1.3777 - accuracy: 0.7616\n",
      "Epoch 140/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.3339 - accuracy: 0.7869\n",
      "Epoch 00140: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 1.3615 - accuracy: 0.7682\n",
      "Epoch 141/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.3548 - accuracy: 0.7642\n",
      "Epoch 00141: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 468us/sample - loss: 1.3450 - accuracy: 0.7616\n",
      "Epoch 142/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.3797 - accuracy: 0.7699\n",
      "Epoch 00142: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 488us/sample - loss: 1.3296 - accuracy: 0.7726\n",
      "Epoch 143/500\n",
      "384/453 [========================>.....] - ETA: 0s - loss: 1.3177 - accuracy: 0.7812\n",
      "Epoch 00143: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 481us/sample - loss: 1.3165 - accuracy: 0.7748\n",
      "Epoch 144/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 1.3077 - accuracy: 0.7723\n",
      "Epoch 00144: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 483us/sample - loss: 1.3062 - accuracy: 0.7748\n",
      "Epoch 145/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.2985 - accuracy: 0.7756\n",
      "Epoch 00145: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 506us/sample - loss: 1.2956 - accuracy: 0.7859\n",
      "Epoch 146/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.2721 - accuracy: 0.7898\n",
      "Epoch 00146: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 457us/sample - loss: 1.2912 - accuracy: 0.7881\n",
      "Epoch 147/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.2679 - accuracy: 0.7812\n",
      "Epoch 00147: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 455us/sample - loss: 1.2749 - accuracy: 0.7792\n",
      "Epoch 148/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.2238 - accuracy: 0.7812\n",
      "Epoch 00148: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 461us/sample - loss: 1.2663 - accuracy: 0.7748\n",
      "Epoch 149/500\n",
      "384/453 [========================>.....] - ETA: 0s - loss: 1.2358 - accuracy: 0.7969\n",
      "Epoch 00149: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 411us/sample - loss: 1.2449 - accuracy: 0.7947\n",
      "Epoch 150/500\n",
      "384/453 [========================>.....] - ETA: 0s - loss: 1.2602 - accuracy: 0.7917\n",
      "Epoch 00150: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 415us/sample - loss: 1.2353 - accuracy: 0.8013\n",
      "Epoch 151/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.2322 - accuracy: 0.7898\n",
      "Epoch 00151: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 461us/sample - loss: 1.2196 - accuracy: 0.7947\n",
      "Epoch 152/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.2218 - accuracy: 0.7926\n",
      "Epoch 00152: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 477us/sample - loss: 1.2160 - accuracy: 0.8057\n",
      "Epoch 153/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.2006 - accuracy: 0.8267\n",
      "Epoch 00153: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 453us/sample - loss: 1.2071 - accuracy: 0.8212\n",
      "Epoch 154/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.1899 - accuracy: 0.8182\n",
      "Epoch 00154: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 455us/sample - loss: 1.1954 - accuracy: 0.8146\n",
      "Epoch 155/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.1452 - accuracy: 0.8239\n",
      "Epoch 00155: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 455us/sample - loss: 1.1797 - accuracy: 0.8124\n",
      "Epoch 156/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.1800 - accuracy: 0.8011\n",
      "Epoch 00156: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 433us/sample - loss: 1.1740 - accuracy: 0.8124\n",
      "Epoch 157/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.1940 - accuracy: 0.8125\n",
      "Epoch 00157: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 457us/sample - loss: 1.1573 - accuracy: 0.8256\n",
      "Epoch 158/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.1351 - accuracy: 0.8295\n",
      "Epoch 00158: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 479us/sample - loss: 1.1415 - accuracy: 0.8322\n",
      "Epoch 159/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.1306 - accuracy: 0.8324\n",
      "Epoch 00159: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 494us/sample - loss: 1.1308 - accuracy: 0.8300\n",
      "Epoch 160/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 1.1684 - accuracy: 0.8062\n",
      "Epoch 00160: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 479us/sample - loss: 1.1289 - accuracy: 0.8256\n",
      "Epoch 161/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.0859 - accuracy: 0.8352\n",
      "Epoch 00161: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 468us/sample - loss: 1.1161 - accuracy: 0.8190\n",
      "Epoch 162/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 1.1035 - accuracy: 0.8281\n",
      "Epoch 00162: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 528us/sample - loss: 1.0971 - accuracy: 0.8300\n",
      "Epoch 163/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 1.0354 - accuracy: 0.82 - ETA: 0s - loss: 1.0827 - accuracy: 0.8313\n",
      "Epoch 00163: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 464us/sample - loss: 1.0864 - accuracy: 0.8234\n",
      "Epoch 164/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.1109 - accuracy: 0.8239\n",
      "Epoch 00164: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 446us/sample - loss: 1.0814 - accuracy: 0.8322\n",
      "Epoch 165/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.0629 - accuracy: 0.8324\n",
      "Epoch 00165: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 464us/sample - loss: 1.0683 - accuracy: 0.8278\n",
      "Epoch 166/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.0659 - accuracy: 0.8466\n",
      "Epoch 00166: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 442us/sample - loss: 1.0702 - accuracy: 0.8300\n",
      "Epoch 167/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.0568 - accuracy: 0.8409\n",
      "Epoch 00167: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 1.0611 - accuracy: 0.8366\n",
      "Epoch 168/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.0793 - accuracy: 0.8239\n",
      "Epoch 00168: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 450us/sample - loss: 1.0570 - accuracy: 0.8411\n",
      "Epoch 169/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.0557 - accuracy: 0.8438\n",
      "Epoch 00169: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 1.0530 - accuracy: 0.8411\n",
      "Epoch 170/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.0177 - accuracy: 0.8466\n",
      "Epoch 00170: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 1.0336 - accuracy: 0.8455\n",
      "Epoch 171/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 1.0402 - accuracy: 0.8438\n",
      "Epoch 00171: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 510us/sample - loss: 1.0415 - accuracy: 0.8433\n",
      "Epoch 172/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.0483 - accuracy: 0.8295\n",
      "Epoch 00172: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 468us/sample - loss: 1.0191 - accuracy: 0.8477\n",
      "Epoch 173/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.9964 - accuracy: 0.8551\n",
      "Epoch 00173: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 446us/sample - loss: 1.0138 - accuracy: 0.8455\n",
      "Epoch 174/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 0.9993 - accuracy: 0.8527\n",
      "Epoch 00174: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 514us/sample - loss: 1.0034 - accuracy: 0.8499\n",
      "Epoch 175/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 1.0008 - accuracy: 0.8494\n",
      "Epoch 00175: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 457us/sample - loss: 0.9929 - accuracy: 0.8521\n",
      "Epoch 176/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.9643 - accuracy: 0.8580\n",
      "Epoch 00176: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 464us/sample - loss: 0.9801 - accuracy: 0.8565\n",
      "Epoch 177/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 0.9598 - accuracy: 0.8661\n",
      "Epoch 00177: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 500us/sample - loss: 0.9606 - accuracy: 0.8653\n",
      "Epoch 178/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 0.9509 - accuracy: 0.8683\n",
      "Epoch 00178: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 517us/sample - loss: 0.9505 - accuracy: 0.8698\n",
      "Epoch 179/500\n",
      "416/453 [==========================>...] - ETA: 0s - loss: 0.9451 - accuracy: 0.8630\n",
      "Epoch 00179: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 521us/sample - loss: 0.9386 - accuracy: 0.8631\n",
      "Epoch 180/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.9104 - accuracy: 0.8807\n",
      "Epoch 00180: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 468us/sample - loss: 0.9279 - accuracy: 0.8675\n",
      "Epoch 181/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 0.9006 - accuracy: 0.8844\n",
      "Epoch 00181: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 466us/sample - loss: 0.9290 - accuracy: 0.8675\n",
      "Epoch 182/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.9120 - accuracy: 0.8693\n",
      "Epoch 00182: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 450us/sample - loss: 0.9318 - accuracy: 0.8653\n",
      "Epoch 183/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.9151 - accuracy: 0.8665\n",
      "Epoch 00183: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 450us/sample - loss: 0.9130 - accuracy: 0.8698\n",
      "Epoch 184/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.8847 - accuracy: 0.8750\n",
      "Epoch 00184: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 459us/sample - loss: 0.8934 - accuracy: 0.8720\n",
      "Epoch 185/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.8366 - accuracy: 0.8807\n",
      "Epoch 00185: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 0.8821 - accuracy: 0.8698\n",
      "Epoch 186/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 0.8993 - accuracy: 0.8728\n",
      "Epoch 00186: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 519us/sample - loss: 0.9048 - accuracy: 0.8720\n",
      "Epoch 187/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 0.9203 - accuracy: 0.8750\n",
      "Epoch 00187: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 481us/sample - loss: 0.8980 - accuracy: 0.8808\n",
      "Epoch 188/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 0.8720 - accuracy: 0.8906\n",
      "Epoch 00188: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 492us/sample - loss: 0.8993 - accuracy: 0.8720\n",
      "Epoch 189/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 0.8835 - accuracy: 0.8683\n",
      "Epoch 00189: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 512us/sample - loss: 0.8806 - accuracy: 0.8698\n",
      "Epoch 190/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.8920 - accuracy: 0.8636\n",
      "Epoch 00190: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 466us/sample - loss: 0.8796 - accuracy: 0.8675\n",
      "Epoch 191/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.8958 - accuracy: 0.8551\n",
      "Epoch 00191: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 492us/sample - loss: 0.9211 - accuracy: 0.8521\n",
      "Epoch 192/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 0.8656 - accuracy: 0.8750\n",
      "Epoch 00192: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 502us/sample - loss: 0.8705 - accuracy: 0.8720\n",
      "Epoch 193/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 0.8515 - accuracy: 0.8728\n",
      "Epoch 00193: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 514us/sample - loss: 0.8528 - accuracy: 0.8720\n",
      "Epoch 194/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 0.8418 - accuracy: 0.8795\n",
      "Epoch 00194: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 547us/sample - loss: 0.8416 - accuracy: 0.8786\n",
      "Epoch 195/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.7884 - accuracy: 0.8892\n",
      "Epoch 00195: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 475us/sample - loss: 0.8257 - accuracy: 0.8786\n",
      "Epoch 196/500\n",
      "288/453 [==================>...........] - ETA: 0s - loss: 0.8322 - accuracy: 0.8785\n",
      "Epoch 00196: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 483us/sample - loss: 0.8167 - accuracy: 0.8830\n",
      "Epoch 197/500\n",
      "384/453 [========================>.....] - ETA: 0s - loss: 0.7906 - accuracy: 0.8880\n",
      "Epoch 00197: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 397us/sample - loss: 0.8004 - accuracy: 0.8830\n",
      "Epoch 198/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.7825 - accuracy: 0.8977\n",
      "Epoch 00198: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 523us/sample - loss: 0.8104 - accuracy: 0.8896\n",
      "Epoch 199/500\n",
      "416/453 [==========================>...] - ETA: 0s - loss: 0.7825 - accuracy: 0.8870\n",
      "Epoch 00199: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 534us/sample - loss: 0.7852 - accuracy: 0.8874\n",
      "Epoch 200/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.7718 - accuracy: 0.8977\n",
      "Epoch 00200: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 494us/sample - loss: 0.7729 - accuracy: 0.8985\n",
      "Epoch 201/500\n",
      "384/453 [========================>.....] - ETA: 0s - loss: 0.7495 - accuracy: 0.9089\n",
      "Epoch 00201: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 419us/sample - loss: 0.7627 - accuracy: 0.8985\n",
      "Epoch 202/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.7489 - accuracy: 0.9062\n",
      "Epoch 00202: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 427us/sample - loss: 0.7547 - accuracy: 0.9095\n",
      "Epoch 203/500\n",
      "384/453 [========================>.....] - ETA: 0s - loss: 0.7510 - accuracy: 0.9036\n",
      "Epoch 00203: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 441us/sample - loss: 0.7472 - accuracy: 0.9095\n",
      "Epoch 204/500\n",
      "416/453 [==========================>...] - ETA: 0s - loss: 0.7335 - accuracy: 0.9062\n",
      "Epoch 00204: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 406us/sample - loss: 0.7366 - accuracy: 0.9073\n",
      "Epoch 205/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.7327 - accuracy: 0.8977\n",
      "Epoch 00205: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 422us/sample - loss: 0.7298 - accuracy: 0.9029\n",
      "Epoch 206/500\n",
      "416/453 [==========================>...] - ETA: 0s - loss: 0.7360 - accuracy: 0.9062\n",
      "Epoch 00206: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 404us/sample - loss: 0.7240 - accuracy: 0.9117\n",
      "Epoch 207/500\n",
      "288/453 [==================>...........] - ETA: 0s - loss: 0.7419 - accuracy: 0.9132\n",
      "Epoch 00207: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 494us/sample - loss: 0.7210 - accuracy: 0.9095\n",
      "Epoch 208/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 0.7138 - accuracy: 0.9156\n",
      "Epoch 00208: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 499us/sample - loss: 0.7112 - accuracy: 0.9139\n",
      "Epoch 209/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.7196 - accuracy: 0.9119\n",
      "Epoch 00209: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 492us/sample - loss: 0.7051 - accuracy: 0.9161\n",
      "Epoch 210/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.7036 - accuracy: 0.9148\n",
      "Epoch 00210: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 454us/sample - loss: 0.6959 - accuracy: 0.9139\n",
      "Epoch 211/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.6891 - accuracy: 0.9062\n",
      "Epoch 00211: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 483us/sample - loss: 0.6910 - accuracy: 0.9117\n",
      "Epoch 212/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 0.6906 - accuracy: 0.9129\n",
      "Epoch 00212: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 503us/sample - loss: 0.6854 - accuracy: 0.9139\n",
      "Epoch 213/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.6817 - accuracy: 0.9148\n",
      "Epoch 00213: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 446us/sample - loss: 0.6780 - accuracy: 0.9161\n",
      "Epoch 214/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 0.6202 - accuracy: 0.9344\n",
      "Epoch 00214: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 481us/sample - loss: 0.6705 - accuracy: 0.9139\n",
      "Epoch 215/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 0.6757 - accuracy: 0.9125\n",
      "Epoch 00215: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 455us/sample - loss: 0.6658 - accuracy: 0.9139\n",
      "Epoch 216/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.6596 - accuracy: 0.9119\n",
      "Epoch 00216: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 457us/sample - loss: 0.6601 - accuracy: 0.9139\n",
      "Epoch 217/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.6169 - accuracy: 0.9233\n",
      "Epoch 00217: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 468us/sample - loss: 0.6526 - accuracy: 0.9183\n",
      "Epoch 218/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 0.6482 - accuracy: 0.9094\n",
      "Epoch 00218: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 512us/sample - loss: 0.6477 - accuracy: 0.9139\n",
      "Epoch 219/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.6657 - accuracy: 0.9091\n",
      "Epoch 00219: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 430us/sample - loss: 0.6429 - accuracy: 0.9161\n",
      "Epoch 220/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.6414 - accuracy: 0.9318\n",
      "Epoch 00220: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 472us/sample - loss: 0.6357 - accuracy: 0.9183\n",
      "Epoch 221/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 0.6322 - accuracy: 0.9250\n",
      "Epoch 00221: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 468us/sample - loss: 0.6295 - accuracy: 0.9183\n",
      "Epoch 222/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 0.6216 - accuracy: 0.9187\n",
      "Epoch 00222: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 470us/sample - loss: 0.6249 - accuracy: 0.9249\n",
      "Epoch 223/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.6348 - accuracy: 0.9205\n",
      "Epoch 00223: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 506us/sample - loss: 0.6170 - accuracy: 0.9205\n",
      "Epoch 224/500\n",
      "288/453 [==================>...........] - ETA: 0s - loss: 0.6380 - accuracy: 0.9201\n",
      "Epoch 00224: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 501us/sample - loss: 0.6148 - accuracy: 0.9227\n",
      "Epoch 225/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 0.6108 - accuracy: 0.9263\n",
      "Epoch 00225: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 510us/sample - loss: 0.6108 - accuracy: 0.9249\n",
      "Epoch 226/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.6183 - accuracy: 0.9261\n",
      "Epoch 00226: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 483us/sample - loss: 0.6056 - accuracy: 0.9227\n",
      "Epoch 227/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.6223 - accuracy: 0.9261\n",
      "Epoch 00227: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 437us/sample - loss: 0.6016 - accuracy: 0.9272\n",
      "Epoch 228/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.5776 - accuracy: 0.9261\n",
      "Epoch 00228: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 499us/sample - loss: 0.5941 - accuracy: 0.9294\n",
      "Epoch 229/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.5937 - accuracy: 0.9205\n",
      "Epoch 00229: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 464us/sample - loss: 0.5898 - accuracy: 0.9294\n",
      "Epoch 230/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.6104 - accuracy: 0.9290\n",
      "Epoch 00230: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 450us/sample - loss: 0.5834 - accuracy: 0.9338\n",
      "Epoch 231/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.5705 - accuracy: 0.9318\n",
      "Epoch 00231: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 472us/sample - loss: 0.5770 - accuracy: 0.9316\n",
      "Epoch 232/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.5738 - accuracy: 0.9347\n",
      "Epoch 00232: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 481us/sample - loss: 0.5740 - accuracy: 0.9294\n",
      "Epoch 233/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 0.5687 - accuracy: 0.9312\n",
      "Epoch 00233: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 464us/sample - loss: 0.5698 - accuracy: 0.9316\n",
      "Epoch 234/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.5570 - accuracy: 0.9347\n",
      "Epoch 00234: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 486us/sample - loss: 0.5642 - accuracy: 0.9316\n",
      "Epoch 235/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.5611 - accuracy: 0.9347\n",
      "Epoch 00235: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 457us/sample - loss: 0.5613 - accuracy: 0.9338\n",
      "Epoch 236/500\n",
      "384/453 [========================>.....] - ETA: 0s - loss: 0.5402 - accuracy: 0.9479\n",
      "Epoch 00236: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 450us/sample - loss: 0.5523 - accuracy: 0.9360\n",
      "Epoch 237/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.5257 - accuracy: 0.9545\n",
      "Epoch 00237: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 439us/sample - loss: 0.5463 - accuracy: 0.9382\n",
      "Epoch 238/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.5381 - accuracy: 0.9403\n",
      "Epoch 00238: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 470us/sample - loss: 0.5435 - accuracy: 0.9382\n",
      "Epoch 239/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4972 - accuracy: 0.9517\n",
      "Epoch 00239: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 444us/sample - loss: 0.5388 - accuracy: 0.9382\n",
      "Epoch 240/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.5326 - accuracy: 0.9403\n",
      "Epoch 00240: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 453us/sample - loss: 0.5362 - accuracy: 0.9360\n",
      "Epoch 241/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.5500 - accuracy: 0.9205\n",
      "Epoch 00241: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 439us/sample - loss: 0.5349 - accuracy: 0.9316\n",
      "Epoch 242/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.5463 - accuracy: 0.9347\n",
      "Epoch 00242: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 437us/sample - loss: 0.5271 - accuracy: 0.9360\n",
      "Epoch 243/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.5066 - accuracy: 0.9460\n",
      "Epoch 00243: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 0.5218 - accuracy: 0.9382\n",
      "Epoch 244/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.5204 - accuracy: 0.9347\n",
      "Epoch 00244: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 475us/sample - loss: 0.5162 - accuracy: 0.9338\n",
      "Epoch 245/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.5280 - accuracy: 0.9318\n",
      "Epoch 00245: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 457us/sample - loss: 0.5108 - accuracy: 0.9382\n",
      "Epoch 246/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.5088 - accuracy: 0.9375\n",
      "Epoch 00246: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 446us/sample - loss: 0.5084 - accuracy: 0.9338\n",
      "Epoch 247/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4987 - accuracy: 0.9375\n",
      "Epoch 00247: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 450us/sample - loss: 0.5039 - accuracy: 0.9360\n",
      "Epoch 248/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4787 - accuracy: 0.9375\n",
      "Epoch 00248: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 0.4993 - accuracy: 0.9338\n",
      "Epoch 249/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4975 - accuracy: 0.9318\n",
      "Epoch 00249: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 464us/sample - loss: 0.4958 - accuracy: 0.9360\n",
      "Epoch 250/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4800 - accuracy: 0.9403\n",
      "Epoch 00250: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 470us/sample - loss: 0.4895 - accuracy: 0.9404\n",
      "Epoch 251/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4693 - accuracy: 0.9517\n",
      "Epoch 00251: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 466us/sample - loss: 0.4859 - accuracy: 0.9426\n",
      "Epoch 252/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4745 - accuracy: 0.9432\n",
      "Epoch 00252: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 0.4837 - accuracy: 0.9382\n",
      "Epoch 253/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4882 - accuracy: 0.9261\n",
      "Epoch 00253: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 0.4791 - accuracy: 0.9360\n",
      "Epoch 254/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4602 - accuracy: 0.9460\n",
      "Epoch 00254: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 0.4747 - accuracy: 0.9382\n",
      "Epoch 255/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4796 - accuracy: 0.9460\n",
      "Epoch 00255: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 446us/sample - loss: 0.4727 - accuracy: 0.9426\n",
      "Epoch 256/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4631 - accuracy: 0.9489\n",
      "Epoch 00256: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 457us/sample - loss: 0.4682 - accuracy: 0.9404\n",
      "Epoch 257/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4882 - accuracy: 0.9261\n",
      "Epoch 00257: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 441us/sample - loss: 0.4632 - accuracy: 0.9404\n",
      "Epoch 258/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4652 - accuracy: 0.9375\n",
      "Epoch 00258: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 459us/sample - loss: 0.4582 - accuracy: 0.9382\n",
      "Epoch 259/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4577 - accuracy: 0.9432\n",
      "Epoch 00259: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 457us/sample - loss: 0.4540 - accuracy: 0.9426\n",
      "Epoch 260/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4488 - accuracy: 0.93 - ETA: 0s - loss: 0.4568 - accuracy: 0.9375\n",
      "Epoch 00260: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 450us/sample - loss: 0.4511 - accuracy: 0.9404\n",
      "Epoch 261/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4376 - accuracy: 0.9489\n",
      "Epoch 00261: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 446us/sample - loss: 0.4482 - accuracy: 0.9382\n",
      "Epoch 262/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4279 - accuracy: 0.9517\n",
      "Epoch 00262: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 442us/sample - loss: 0.4433 - accuracy: 0.9448\n",
      "Epoch 263/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4588 - accuracy: 0.9375\n",
      "Epoch 00263: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 470us/sample - loss: 0.4394 - accuracy: 0.9426\n",
      "Epoch 264/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4431 - accuracy: 0.9403\n",
      "Epoch 00264: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 444us/sample - loss: 0.4371 - accuracy: 0.9426\n",
      "Epoch 265/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4323 - accuracy: 0.9375\n",
      "Epoch 00265: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 457us/sample - loss: 0.4335 - accuracy: 0.9404\n",
      "Epoch 266/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4096 - accuracy: 0.9517\n",
      "Epoch 00266: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 451us/sample - loss: 0.4289 - accuracy: 0.9426\n",
      "Epoch 267/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4350 - accuracy: 0.9347\n",
      "Epoch 00267: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 459us/sample - loss: 0.4270 - accuracy: 0.9382\n",
      "Epoch 268/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4166 - accuracy: 0.9375\n",
      "Epoch 00268: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 797us/sample - loss: 0.4240 - accuracy: 0.9404\n",
      "Epoch 269/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 0.4026 - accuracy: 0.9438\n",
      "Epoch 00269: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 466us/sample - loss: 0.4198 - accuracy: 0.9382\n",
      "Epoch 270/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 0.4148 - accuracy: 0.9406\n",
      "Epoch 00270: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 494us/sample - loss: 0.4161 - accuracy: 0.9382\n",
      "Epoch 271/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4357 - accuracy: 0.9318\n",
      "Epoch 00271: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 444us/sample - loss: 0.4119 - accuracy: 0.9426\n",
      "Epoch 272/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4080 - accuracy: 0.9403\n",
      "Epoch 00272: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 446us/sample - loss: 0.4089 - accuracy: 0.9404\n",
      "Epoch 273/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 0.4198 - accuracy: 0.9375\n",
      "Epoch 00273: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 455us/sample - loss: 0.4049 - accuracy: 0.9404\n",
      "Epoch 274/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.3897 - accuracy: 0.9489\n",
      "Epoch 00274: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 444us/sample - loss: 0.4032 - accuracy: 0.9426\n",
      "Epoch 275/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.3876 - accuracy: 0.9460\n",
      "Epoch 00275: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 446us/sample - loss: 0.4007 - accuracy: 0.9426\n",
      "Epoch 276/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4057 - accuracy: 0.9375\n",
      "Epoch 00276: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 444us/sample - loss: 0.3996 - accuracy: 0.9448\n",
      "Epoch 277/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4168 - accuracy: 0.9432\n",
      "Epoch 00277: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 457us/sample - loss: 0.3958 - accuracy: 0.9448\n",
      "Epoch 278/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4084 - accuracy: 0.9432\n",
      "Epoch 00278: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 450us/sample - loss: 0.3982 - accuracy: 0.9448\n",
      "Epoch 279/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4019 - accuracy: 0.9403\n",
      "Epoch 00279: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 439us/sample - loss: 0.4010 - accuracy: 0.9382\n",
      "Epoch 280/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 0.4076 - accuracy: 0.9406\n",
      "Epoch 00280: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 450us/sample - loss: 0.3991 - accuracy: 0.9448\n",
      "Epoch 281/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4061 - accuracy: 0.9432\n",
      "Epoch 00281: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 453us/sample - loss: 0.4039 - accuracy: 0.9404\n",
      "Epoch 282/500\n",
      "384/453 [========================>.....] - ETA: 0s - loss: 0.4358 - accuracy: 0.9271\n",
      "Epoch 00282: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 415us/sample - loss: 0.4251 - accuracy: 0.9338\n",
      "Epoch 283/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4451 - accuracy: 0.9290\n",
      "Epoch 00283: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 457us/sample - loss: 0.4296 - accuracy: 0.9316\n",
      "Epoch 284/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4213 - accuracy: 0.9403\n",
      "Epoch 00284: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 444us/sample - loss: 0.4130 - accuracy: 0.9338\n",
      "Epoch 285/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4169 - accuracy: 0.9375\n",
      "Epoch 00285: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 439us/sample - loss: 0.4131 - accuracy: 0.9382\n",
      "Epoch 286/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4494 - accuracy: 0.9233\n",
      "Epoch 00286: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 439us/sample - loss: 0.4412 - accuracy: 0.9227\n",
      "Epoch 287/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4405 - accuracy: 0.9148\n",
      "Epoch 00287: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 457us/sample - loss: 0.4370 - accuracy: 0.9183\n",
      "Epoch 288/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4881 - accuracy: 0.8977\n",
      "Epoch 00288: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 483us/sample - loss: 0.4902 - accuracy: 0.8985\n",
      "Epoch 289/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4975 - accuracy: 0.8949\n",
      "Epoch 00289: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 453us/sample - loss: 0.5121 - accuracy: 0.8896\n",
      "Epoch 290/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4754 - accuracy: 0.9176\n",
      "Epoch 00290: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 455us/sample - loss: 0.4760 - accuracy: 0.9183\n",
      "Epoch 291/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4714 - accuracy: 0.9119\n",
      "Epoch 00291: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 449us/sample - loss: 0.4480 - accuracy: 0.9227\n",
      "Epoch 292/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.4029 - accuracy: 0.9347\n",
      "Epoch 00292: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 482us/sample - loss: 0.4251 - accuracy: 0.9338\n",
      "Epoch 293/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 0.3957 - accuracy: 0.9312\n",
      "Epoch 00293: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 510us/sample - loss: 0.4123 - accuracy: 0.9360\n",
      "Epoch 294/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 0.3962 - accuracy: 0.9353\n",
      "Epoch 00294: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 508us/sample - loss: 0.3956 - accuracy: 0.9360\n",
      "Epoch 295/500\n",
      "384/453 [========================>.....] - ETA: 0s - loss: 0.3696 - accuracy: 0.9479\n",
      "Epoch 00295: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 418us/sample - loss: 0.3812 - accuracy: 0.9426\n",
      "Epoch 296/500\n",
      "416/453 [==========================>...] - ETA: 0s - loss: 0.3619 - accuracy: 0.9495\n",
      "Epoch 00296: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 431us/sample - loss: 0.3753 - accuracy: 0.9448\n",
      "Epoch 297/500\n",
      "416/453 [==========================>...] - ETA: 0s - loss: 0.3612 - accuracy: 0.9495\n",
      "Epoch 00297: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 417us/sample - loss: 0.3656 - accuracy: 0.9426\n",
      "Epoch 298/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.3500 - accuracy: 0.9432\n",
      "Epoch 00298: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 457us/sample - loss: 0.3575 - accuracy: 0.9426\n",
      "Epoch 299/500\n",
      "384/453 [========================>.....] - ETA: 0s - loss: 0.3571 - accuracy: 0.9427\n",
      "Epoch 00299: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 402us/sample - loss: 0.3503 - accuracy: 0.9470\n",
      "Epoch 300/500\n",
      "416/453 [==========================>...] - ETA: 0s - loss: 0.3473 - accuracy: 0.9423\n",
      "Epoch 00300: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 402us/sample - loss: 0.3470 - accuracy: 0.9448\n",
      "Epoch 301/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.3311 - accuracy: 0.9574\n",
      "Epoch 00301: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 455us/sample - loss: 0.3428 - accuracy: 0.9492\n",
      "Epoch 302/500\n",
      "384/453 [========================>.....] - ETA: 0s - loss: 0.3411 - accuracy: 0.9453\n",
      "Epoch 00302: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 433us/sample - loss: 0.3419 - accuracy: 0.9448\n",
      "Epoch 303/500\n",
      "384/453 [========================>.....] - ETA: 0s - loss: 0.3389 - accuracy: 0.9453\n",
      "Epoch 00303: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 442us/sample - loss: 0.3404 - accuracy: 0.9470\n",
      "Epoch 304/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.3337 - accuracy: 0.9460\n",
      "Epoch 00304: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 461us/sample - loss: 0.3376 - accuracy: 0.9470\n",
      "Epoch 305/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.3462 - accuracy: 0.9460\n",
      "Epoch 00305: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 479us/sample - loss: 0.3372 - accuracy: 0.9448\n",
      "Epoch 306/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 0.3515 - accuracy: 0.9438\n",
      "Epoch 00306: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 477us/sample - loss: 0.3361 - accuracy: 0.9470\n",
      "Epoch 307/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.3162 - accuracy: 0.9574\n",
      "Epoch 00307: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 466us/sample - loss: 0.3352 - accuracy: 0.9470\n",
      "Epoch 308/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.3336 - accuracy: 0.9432\n",
      "Epoch 00308: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 446us/sample - loss: 0.3328 - accuracy: 0.9448\n",
      "Epoch 309/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.3361 - accuracy: 0.9375\n",
      "Epoch 00309: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 442us/sample - loss: 0.3332 - accuracy: 0.9426\n",
      "Epoch 310/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.3131 - accuracy: 0.9517\n",
      "Epoch 00310: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 444us/sample - loss: 0.3250 - accuracy: 0.9448\n",
      "Epoch 311/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.3186 - accuracy: 0.9545\n",
      "Epoch 00311: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 439us/sample - loss: 0.3192 - accuracy: 0.9492\n",
      "Epoch 312/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.3373 - accuracy: 0.9375\n",
      "Epoch 00312: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 446us/sample - loss: 0.3159 - accuracy: 0.9426\n",
      "Epoch 313/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.3164 - accuracy: 0.9545\n",
      "Epoch 00313: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 441us/sample - loss: 0.3118 - accuracy: 0.9470\n",
      "Epoch 314/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.3000 - accuracy: 0.9432\n",
      "Epoch 00314: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 444us/sample - loss: 0.3088 - accuracy: 0.9470\n",
      "Epoch 315/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.3147 - accuracy: 0.9432\n",
      "Epoch 00315: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 0.3073 - accuracy: 0.9470\n",
      "Epoch 316/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.3193 - accuracy: 0.9347\n",
      "Epoch 00316: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 461us/sample - loss: 0.3065 - accuracy: 0.9470\n",
      "Epoch 317/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.3079 - accuracy: 0.9460\n",
      "Epoch 00317: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 0.3023 - accuracy: 0.9470\n",
      "Epoch 318/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.3032 - accuracy: 0.9489\n",
      "Epoch 00318: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 497us/sample - loss: 0.2997 - accuracy: 0.9514\n",
      "Epoch 319/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2907 - accuracy: 0.9489\n",
      "Epoch 00319: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 514us/sample - loss: 0.2979 - accuracy: 0.9448\n",
      "Epoch 320/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2959 - accuracy: 0.9602\n",
      "Epoch 00320: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 459us/sample - loss: 0.2960 - accuracy: 0.9514\n",
      "Epoch 321/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.3040 - accuracy: 0.9460\n",
      "Epoch 00321: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 464us/sample - loss: 0.2945 - accuracy: 0.9448\n",
      "Epoch 322/500\n",
      "384/453 [========================>.....] - ETA: 0s - loss: 0.2826 - accuracy: 0.9505\n",
      "Epoch 00322: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 442us/sample - loss: 0.2928 - accuracy: 0.9470\n",
      "Epoch 323/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2766 - accuracy: 0.9489\n",
      "Epoch 00323: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 492us/sample - loss: 0.2930 - accuracy: 0.9470\n",
      "Epoch 324/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2882 - accuracy: 0.9545\n",
      "Epoch 00324: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 464us/sample - loss: 0.2889 - accuracy: 0.9514\n",
      "Epoch 325/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 0.3000 - accuracy: 0.9438\n",
      "Epoch 00325: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 550us/sample - loss: 0.2862 - accuracy: 0.9492\n",
      "Epoch 326/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 0.2855 - accuracy: 0.9487\n",
      "Epoch 00326: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 506us/sample - loss: 0.2867 - accuracy: 0.9470\n",
      "Epoch 327/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 0.2761 - accuracy: 0.9500\n",
      "Epoch 00327: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 475us/sample - loss: 0.2840 - accuracy: 0.9492\n",
      "Epoch 328/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2755 - accuracy: 0.9545\n",
      "Epoch 00328: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 472us/sample - loss: 0.2799 - accuracy: 0.9514\n",
      "Epoch 329/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 0.2776 - accuracy: 0.9487\n",
      "Epoch 00329: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 499us/sample - loss: 0.2782 - accuracy: 0.9470\n",
      "Epoch 330/500\n",
      "416/453 [==========================>...] - ETA: 0s - loss: 0.2783 - accuracy: 0.9495\n",
      "Epoch 00330: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 554us/sample - loss: 0.2757 - accuracy: 0.9492\n",
      "Epoch 331/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 0.2749 - accuracy: 0.9487\n",
      "Epoch 00331: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 512us/sample - loss: 0.2737 - accuracy: 0.9492\n",
      "Epoch 332/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 0.2613 - accuracy: 0.9625\n",
      "Epoch 00332: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 486us/sample - loss: 0.2712 - accuracy: 0.9492\n",
      "Epoch 333/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2570 - accuracy: 0.9574\n",
      "Epoch 00333: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 450us/sample - loss: 0.2696 - accuracy: 0.9470\n",
      "Epoch 334/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2635 - accuracy: 0.9489\n",
      "Epoch 00334: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 494us/sample - loss: 0.2679 - accuracy: 0.9492\n",
      "Epoch 335/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 0.2674 - accuracy: 0.9509\n",
      "Epoch 00335: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 494us/sample - loss: 0.2662 - accuracy: 0.9514\n",
      "Epoch 336/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2612 - accuracy: 0.9517\n",
      "Epoch 00336: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 439us/sample - loss: 0.2647 - accuracy: 0.9470\n",
      "Epoch 337/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2655 - accuracy: 0.9489\n",
      "Epoch 00337: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 450us/sample - loss: 0.2626 - accuracy: 0.9492\n",
      "Epoch 338/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2642 - accuracy: 0.9460\n",
      "Epoch 00338: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 0.2613 - accuracy: 0.9492\n",
      "Epoch 339/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2629 - accuracy: 0.9432 ETA: 0s - loss: 0.2762 - accuracy: 0.93\n",
      "Epoch 00339: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 444us/sample - loss: 0.2593 - accuracy: 0.9470\n",
      "Epoch 340/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2462 - accuracy: 0.9574\n",
      "Epoch 00340: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 470us/sample - loss: 0.2577 - accuracy: 0.9470\n",
      "Epoch 341/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 0.2536 - accuracy: 0.9509\n",
      "Epoch 00341: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 501us/sample - loss: 0.2565 - accuracy: 0.9492\n",
      "Epoch 342/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2459 - accuracy: 0.9489\n",
      "Epoch 00342: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 453us/sample - loss: 0.2554 - accuracy: 0.9514\n",
      "Epoch 343/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2476 - accuracy: 0.9545\n",
      "Epoch 00343: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 444us/sample - loss: 0.2547 - accuracy: 0.9514\n",
      "Epoch 344/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2604 - accuracy: 0.9517\n",
      "Epoch 00344: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 464us/sample - loss: 0.2521 - accuracy: 0.9514\n",
      "Epoch 345/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2456 - accuracy: 0.9545\n",
      "Epoch 00345: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 428us/sample - loss: 0.2505 - accuracy: 0.9514\n",
      "Epoch 346/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2514 - accuracy: 0.9517\n",
      "Epoch 00346: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 455us/sample - loss: 0.2482 - accuracy: 0.9514\n",
      "Epoch 347/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2385 - accuracy: 0.9489\n",
      "Epoch 00347: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 439us/sample - loss: 0.2472 - accuracy: 0.9492\n",
      "Epoch 348/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2310 - accuracy: 0.9545\n",
      "Epoch 00348: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 444us/sample - loss: 0.2458 - accuracy: 0.9514\n",
      "Epoch 349/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2309 - accuracy: 0.9545\n",
      "Epoch 00349: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 466us/sample - loss: 0.2453 - accuracy: 0.9448\n",
      "Epoch 350/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2619 - accuracy: 0.9460\n",
      "Epoch 00350: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 444us/sample - loss: 0.2454 - accuracy: 0.9514\n",
      "Epoch 351/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2439 - accuracy: 0.9460\n",
      "Epoch 00351: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 446us/sample - loss: 0.2423 - accuracy: 0.9470\n",
      "Epoch 352/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2396 - accuracy: 0.9545\n",
      "Epoch 00352: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 450us/sample - loss: 0.2404 - accuracy: 0.9514\n",
      "Epoch 353/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2325 - accuracy: 0.9631\n",
      "Epoch 00353: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 446us/sample - loss: 0.2393 - accuracy: 0.9514\n",
      "Epoch 354/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2252 - accuracy: 0.9631\n",
      "Epoch 00354: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 461us/sample - loss: 0.2380 - accuracy: 0.9514\n",
      "Epoch 355/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2490 - accuracy: 0.9460\n",
      "Epoch 00355: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 446us/sample - loss: 0.2370 - accuracy: 0.9514\n",
      "Epoch 356/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2681 - accuracy: 0.9347\n",
      "Epoch 00356: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 444us/sample - loss: 0.2360 - accuracy: 0.9492\n",
      "Epoch 357/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2304 - accuracy: 0.9460\n",
      "Epoch 00357: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 444us/sample - loss: 0.2327 - accuracy: 0.9492\n",
      "Epoch 358/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2346 - accuracy: 0.9432\n",
      "Epoch 00358: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 0.2309 - accuracy: 0.9470\n",
      "Epoch 359/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2324 - accuracy: 0.9460\n",
      "Epoch 00359: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 481us/sample - loss: 0.2303 - accuracy: 0.9492\n",
      "Epoch 360/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2340 - accuracy: 0.9489\n",
      "Epoch 00360: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 0.2282 - accuracy: 0.9514\n",
      "Epoch 361/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2300 - accuracy: 0.9460\n",
      "Epoch 00361: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 437us/sample - loss: 0.2268 - accuracy: 0.9492\n",
      "Epoch 362/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2336 - accuracy: 0.9460\n",
      "Epoch 00362: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 422us/sample - loss: 0.2249 - accuracy: 0.9470\n",
      "Epoch 363/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2171 - accuracy: 0.9545\n",
      "Epoch 00363: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 457us/sample - loss: 0.2239 - accuracy: 0.9470\n",
      "Epoch 364/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 0.2103 - accuracy: 0.9563\n",
      "Epoch 00364: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 486us/sample - loss: 0.2224 - accuracy: 0.9514\n",
      "Epoch 365/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2154 - accuracy: 0.9517\n",
      "Epoch 00365: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 0.2209 - accuracy: 0.9470\n",
      "Epoch 366/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2163 - accuracy: 0.9460\n",
      "Epoch 00366: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 444us/sample - loss: 0.2206 - accuracy: 0.9448\n",
      "Epoch 367/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 0.2204 - accuracy: 0.9487\n",
      "Epoch 00367: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 481us/sample - loss: 0.2188 - accuracy: 0.9492\n",
      "Epoch 368/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2043 - accuracy: 0.9545\n",
      "Epoch 00368: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 444us/sample - loss: 0.2174 - accuracy: 0.9470\n",
      "Epoch 369/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2132 - accuracy: 0.9545\n",
      "Epoch 00369: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 446us/sample - loss: 0.2188 - accuracy: 0.9470\n",
      "Epoch 370/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2056 - accuracy: 0.9517\n",
      "Epoch 00370: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 437us/sample - loss: 0.2155 - accuracy: 0.9492\n",
      "Epoch 371/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2213 - accuracy: 0.9432\n",
      "Epoch 00371: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 444us/sample - loss: 0.2149 - accuracy: 0.9470\n",
      "Epoch 372/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2089 - accuracy: 0.9489\n",
      "Epoch 00372: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 435us/sample - loss: 0.2129 - accuracy: 0.9492\n",
      "Epoch 373/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2012 - accuracy: 0.9517\n",
      "Epoch 00373: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 455us/sample - loss: 0.2131 - accuracy: 0.9470\n",
      "Epoch 374/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2077 - accuracy: 0.9460\n",
      "Epoch 00374: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 455us/sample - loss: 0.2113 - accuracy: 0.9448\n",
      "Epoch 375/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2119 - accuracy: 0.9460\n",
      "Epoch 00375: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 455us/sample - loss: 0.2093 - accuracy: 0.9492\n",
      "Epoch 376/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2055 - accuracy: 0.9489\n",
      "Epoch 00376: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 446us/sample - loss: 0.2083 - accuracy: 0.9492\n",
      "Epoch 377/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2086 - accuracy: 0.9489\n",
      "Epoch 00377: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 444us/sample - loss: 0.2077 - accuracy: 0.9448\n",
      "Epoch 378/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 0.2077 - accuracy: 0.9487\n",
      "Epoch 00378: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 534us/sample - loss: 0.2063 - accuracy: 0.9492\n",
      "Epoch 379/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2045 - accuracy: 0.9489\n",
      "Epoch 00379: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 481us/sample - loss: 0.2053 - accuracy: 0.9470\n",
      "Epoch 380/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 0.1792 - accuracy: 0.9594\n",
      "Epoch 00380: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 481us/sample - loss: 0.2042 - accuracy: 0.9470\n",
      "Epoch 381/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2031 - accuracy: 0.9517\n",
      "Epoch 00381: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 457us/sample - loss: 0.2030 - accuracy: 0.9492\n",
      "Epoch 382/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2053 - accuracy: 0.9460\n",
      "Epoch 00382: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 461us/sample - loss: 0.2025 - accuracy: 0.9470\n",
      "Epoch 383/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1943 - accuracy: 0.9517\n",
      "Epoch 00383: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 455us/sample - loss: 0.2011 - accuracy: 0.9536\n",
      "Epoch 384/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1958 - accuracy: 0.9517\n",
      "Epoch 00384: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 472us/sample - loss: 0.2005 - accuracy: 0.9470\n",
      "Epoch 385/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 0.1937 - accuracy: 0.9509\n",
      "Epoch 00385: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 503us/sample - loss: 0.1983 - accuracy: 0.9492\n",
      "Epoch 386/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2028 - accuracy: 0.9432\n",
      "Epoch 00386: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 514us/sample - loss: 0.1992 - accuracy: 0.9470\n",
      "Epoch 387/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 0.1917 - accuracy: 0.9469\n",
      "Epoch 00387: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 472us/sample - loss: 0.1969 - accuracy: 0.9492\n",
      "Epoch 388/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1930 - accuracy: 0.9489\n",
      "Epoch 00388: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 453us/sample - loss: 0.1961 - accuracy: 0.9492\n",
      "Epoch 389/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1935 - accuracy: 0.9517\n",
      "Epoch 00389: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 0.1948 - accuracy: 0.9536\n",
      "Epoch 390/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1858 - accuracy: 0.9545\n",
      "Epoch 00390: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 444us/sample - loss: 0.1938 - accuracy: 0.9514\n",
      "Epoch 391/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1982 - accuracy: 0.9545\n",
      "Epoch 00391: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 0.1936 - accuracy: 0.9536\n",
      "Epoch 392/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1958 - accuracy: 0.9489 ETA: 0s - loss: 0.1693 - accuracy: 0.96\n",
      "Epoch 00392: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 459us/sample - loss: 0.1929 - accuracy: 0.9492\n",
      "Epoch 393/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1879 - accuracy: 0.9545\n",
      "Epoch 00393: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 464us/sample - loss: 0.1913 - accuracy: 0.9492\n",
      "Epoch 394/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1948 - accuracy: 0.9489\n",
      "Epoch 00394: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 512us/sample - loss: 0.1913 - accuracy: 0.9448\n",
      "Epoch 395/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1973 - accuracy: 0.9432\n",
      "Epoch 00395: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 437us/sample - loss: 0.1898 - accuracy: 0.9514\n",
      "Epoch 396/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1913 - accuracy: 0.9489\n",
      "Epoch 00396: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 461us/sample - loss: 0.1888 - accuracy: 0.9470\n",
      "Epoch 397/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 0.1894 - accuracy: 0.9487\n",
      "Epoch 00397: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 567us/sample - loss: 0.1881 - accuracy: 0.9492\n",
      "Epoch 398/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1980 - accuracy: 0.9545\n",
      "Epoch 00398: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 499us/sample - loss: 0.1880 - accuracy: 0.9514\n",
      "Epoch 399/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1825 - accuracy: 0.9545\n",
      "Epoch 00399: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 475us/sample - loss: 0.1867 - accuracy: 0.9514\n",
      "Epoch 400/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1931 - accuracy: 0.9489\n",
      "Epoch 00400: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 439us/sample - loss: 0.1881 - accuracy: 0.9514\n",
      "Epoch 401/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1852 - accuracy: 0.9545\n",
      "Epoch 00401: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 0.1874 - accuracy: 0.9492\n",
      "Epoch 402/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1973 - accuracy: 0.9460\n",
      "Epoch 00402: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 457us/sample - loss: 0.1867 - accuracy: 0.9470\n",
      "Epoch 403/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 0.1820 - accuracy: 0.9500\n",
      "Epoch 00403: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 466us/sample - loss: 0.1844 - accuracy: 0.9492\n",
      "Epoch 404/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1873 - accuracy: 0.9460\n",
      "Epoch 00404: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 446us/sample - loss: 0.1832 - accuracy: 0.9448\n",
      "Epoch 405/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1800 - accuracy: 0.9574\n",
      "Epoch 00405: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 457us/sample - loss: 0.1816 - accuracy: 0.9536\n",
      "Epoch 406/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1917 - accuracy: 0.9403\n",
      "Epoch 00406: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 459us/sample - loss: 0.1801 - accuracy: 0.9492\n",
      "Epoch 407/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 0.1805 - accuracy: 0.9509\n",
      "Epoch 00407: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 532us/sample - loss: 0.1791 - accuracy: 0.9514\n",
      "Epoch 408/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 0.1803 - accuracy: 0.9509\n",
      "Epoch 00408: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 527us/sample - loss: 0.1791 - accuracy: 0.9514\n",
      "Epoch 409/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2013 - accuracy: 0.9347\n",
      "Epoch 00409: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 486us/sample - loss: 0.1801 - accuracy: 0.9492\n",
      "Epoch 410/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 0.2095 - accuracy: 0.9438\n",
      "Epoch 00410: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 528us/sample - loss: 0.1964 - accuracy: 0.9470\n",
      "Epoch 411/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 0.1970 - accuracy: 0.9500\n",
      "Epoch 00411: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 481us/sample - loss: 0.2145 - accuracy: 0.9426\n",
      "Epoch 412/500\n",
      "416/453 [==========================>...] - ETA: 0s - loss: 0.2081 - accuracy: 0.9495\n",
      "Epoch 00412: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 546us/sample - loss: 0.2177 - accuracy: 0.9426\n",
      "Epoch 413/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2274 - accuracy: 0.9489\n",
      "Epoch 00413: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 481us/sample - loss: 0.2558 - accuracy: 0.9338\n",
      "Epoch 414/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2782 - accuracy: 0.9318\n",
      "Epoch 00414: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 483us/sample - loss: 0.3066 - accuracy: 0.9117\n",
      "Epoch 415/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.3242 - accuracy: 0.9091\n",
      "Epoch 00415: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 465us/sample - loss: 0.3403 - accuracy: 0.9007\n",
      "Epoch 416/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2977 - accuracy: 0.9205\n",
      "Epoch 00416: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 488us/sample - loss: 0.2982 - accuracy: 0.9183\n",
      "Epoch 417/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2883 - accuracy: 0.9233\n",
      "Epoch 00417: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 521us/sample - loss: 0.2921 - accuracy: 0.9227\n",
      "Epoch 418/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 0.2692 - accuracy: 0.9286\n",
      "Epoch 00418: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 525us/sample - loss: 0.2686 - accuracy: 0.9294\n",
      "Epoch 419/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2740 - accuracy: 0.9318\n",
      "Epoch 00419: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 447us/sample - loss: 0.2620 - accuracy: 0.9360\n",
      "Epoch 420/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2420 - accuracy: 0.9403\n",
      "Epoch 00420: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 466us/sample - loss: 0.2442 - accuracy: 0.9382\n",
      "Epoch 421/500\n",
      "448/453 [============================>.] - ETA: 0s - loss: 0.2193 - accuracy: 0.9420\n",
      "Epoch 00421: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 492us/sample - loss: 0.2195 - accuracy: 0.9404\n",
      "Epoch 422/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 0.1912 - accuracy: 0.9469\n",
      "Epoch 00422: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 475us/sample - loss: 0.2052 - accuracy: 0.9404\n",
      "Epoch 423/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1810 - accuracy: 0.9574\n",
      "Epoch 00423: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 461us/sample - loss: 0.2000 - accuracy: 0.9492\n",
      "Epoch 424/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1783 - accuracy: 0.9602\n",
      "Epoch 00424: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 453us/sample - loss: 0.1948 - accuracy: 0.9492\n",
      "Epoch 425/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1929 - accuracy: 0.9432\n",
      "Epoch 00425: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 437us/sample - loss: 0.1902 - accuracy: 0.9448\n",
      "Epoch 426/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1926 - accuracy: 0.9460\n",
      "Epoch 00426: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 455us/sample - loss: 0.1876 - accuracy: 0.9470\n",
      "Epoch 427/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.2003 - accuracy: 0.9517\n",
      "Epoch 00427: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 468us/sample - loss: 0.1848 - accuracy: 0.9514\n",
      "Epoch 428/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1840 - accuracy: 0.9460\n",
      "Epoch 00428: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 0.1819 - accuracy: 0.9492\n",
      "Epoch 429/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1789 - accuracy: 0.9545\n",
      "Epoch 00429: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 446us/sample - loss: 0.1804 - accuracy: 0.9514\n",
      "Epoch 430/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1681 - accuracy: 0.9460\n",
      "Epoch 00430: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 439us/sample - loss: 0.1789 - accuracy: 0.9426\n",
      "Epoch 431/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1832 - accuracy: 0.9347\n",
      "Epoch 00431: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 439us/sample - loss: 0.1770 - accuracy: 0.9426\n",
      "Epoch 432/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1815 - accuracy: 0.9517\n",
      "Epoch 00432: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 437us/sample - loss: 0.1743 - accuracy: 0.9514\n",
      "Epoch 433/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1805 - accuracy: 0.9460\n",
      "Epoch 00433: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 439us/sample - loss: 0.1746 - accuracy: 0.9470\n",
      "Epoch 434/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1725 - accuracy: 0.9517\n",
      "Epoch 00434: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 0.1726 - accuracy: 0.9470\n",
      "Epoch 435/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1739 - accuracy: 0.9517\n",
      "Epoch 00435: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 0.1728 - accuracy: 0.9470\n",
      "Epoch 436/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1833 - accuracy: 0.9347\n",
      "Epoch 00436: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 459us/sample - loss: 0.1703 - accuracy: 0.9470\n",
      "Epoch 437/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1508 - accuracy: 0.9602\n",
      "Epoch 00437: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 459us/sample - loss: 0.1689 - accuracy: 0.9470\n",
      "Epoch 438/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1707 - accuracy: 0.9489\n",
      "Epoch 00438: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 464us/sample - loss: 0.1688 - accuracy: 0.9470\n",
      "Epoch 439/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1689 - accuracy: 0.9517\n",
      "Epoch 00439: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 459us/sample - loss: 0.1676 - accuracy: 0.9536\n",
      "Epoch 440/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1736 - accuracy: 0.9375\n",
      "Epoch 00440: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 437us/sample - loss: 0.1668 - accuracy: 0.9426\n",
      "Epoch 441/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1669 - accuracy: 0.9432\n",
      "Epoch 00441: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 464us/sample - loss: 0.1655 - accuracy: 0.9448\n",
      "Epoch 442/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1672 - accuracy: 0.9489\n",
      "Epoch 00442: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 459us/sample - loss: 0.1646 - accuracy: 0.9470\n",
      "Epoch 443/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1639 - accuracy: 0.9545\n",
      "Epoch 00443: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 437us/sample - loss: 0.1633 - accuracy: 0.9514\n",
      "Epoch 444/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1729 - accuracy: 0.9460\n",
      "Epoch 00444: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 455us/sample - loss: 0.1638 - accuracy: 0.9492\n",
      "Epoch 445/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1641 - accuracy: 0.9545\n",
      "Epoch 00445: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 439us/sample - loss: 0.1614 - accuracy: 0.9536\n",
      "Epoch 446/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1642 - accuracy: 0.9460\n",
      "Epoch 00446: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 450us/sample - loss: 0.1608 - accuracy: 0.9492\n",
      "Epoch 447/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1497 - accuracy: 0.9489\n",
      "Epoch 00447: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 426us/sample - loss: 0.1593 - accuracy: 0.9492\n",
      "Epoch 448/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1695 - accuracy: 0.9489\n",
      "Epoch 00448: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 446us/sample - loss: 0.1586 - accuracy: 0.9514\n",
      "Epoch 449/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1612 - accuracy: 0.9460\n",
      "Epoch 00449: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 461us/sample - loss: 0.1574 - accuracy: 0.9448\n",
      "Epoch 450/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1739 - accuracy: 0.9403\n",
      "Epoch 00450: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 439us/sample - loss: 0.1566 - accuracy: 0.9514\n",
      "Epoch 451/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1481 - accuracy: 0.9517\n",
      "Epoch 00451: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 437us/sample - loss: 0.1555 - accuracy: 0.9448\n",
      "Epoch 452/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1421 - accuracy: 0.9574\n",
      "Epoch 00452: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 453us/sample - loss: 0.1566 - accuracy: 0.9470\n",
      "Epoch 453/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1621 - accuracy: 0.9517\n",
      "Epoch 00453: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 435us/sample - loss: 0.1548 - accuracy: 0.9536\n",
      "Epoch 454/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1541 - accuracy: 0.9545\n",
      "Epoch 00454: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 450us/sample - loss: 0.1549 - accuracy: 0.9514\n",
      "Epoch 455/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1632 - accuracy: 0.9403\n",
      "Epoch 00455: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 444us/sample - loss: 0.1532 - accuracy: 0.9448\n",
      "Epoch 456/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1399 - accuracy: 0.9602\n",
      "Epoch 00456: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 444us/sample - loss: 0.1522 - accuracy: 0.9514\n",
      "Epoch 457/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1431 - accuracy: 0.9545\n",
      "Epoch 00457: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 0.1527 - accuracy: 0.9470\n",
      "Epoch 458/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1626 - accuracy: 0.9460\n",
      "Epoch 00458: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 444us/sample - loss: 0.1513 - accuracy: 0.9492\n",
      "Epoch 459/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1619 - accuracy: 0.9489\n",
      "Epoch 00459: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 446us/sample - loss: 0.1508 - accuracy: 0.9492\n",
      "Epoch 460/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1357 - accuracy: 0.9517\n",
      "Epoch 00460: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 459us/sample - loss: 0.1506 - accuracy: 0.9470\n",
      "Epoch 461/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1494 - accuracy: 0.9432\n",
      "Epoch 00461: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 433us/sample - loss: 0.1510 - accuracy: 0.9448\n",
      "Epoch 462/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1431 - accuracy: 0.9545\n",
      "Epoch 00462: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 0.1495 - accuracy: 0.9536\n",
      "Epoch 463/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 0.1531 - accuracy: 0.9500\n",
      "Epoch 00463: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 488us/sample - loss: 0.1496 - accuracy: 0.9514\n",
      "Epoch 464/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1643 - accuracy: 0.9347\n",
      "Epoch 00464: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 0.1485 - accuracy: 0.9470\n",
      "Epoch 465/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1430 - accuracy: 0.9659\n",
      "Epoch 00465: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 0.1486 - accuracy: 0.9536\n",
      "Epoch 466/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1539 - accuracy: 0.9432\n",
      "Epoch 00466: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 0.1473 - accuracy: 0.9470\n",
      "Epoch 467/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1581 - accuracy: 0.9432\n",
      "Epoch 00467: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 439us/sample - loss: 0.1470 - accuracy: 0.9470\n",
      "Epoch 468/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1482 - accuracy: 0.9545\n",
      "Epoch 00468: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 457us/sample - loss: 0.1468 - accuracy: 0.9514\n",
      "Epoch 469/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1410 - accuracy: 0.9517\n",
      "Epoch 00469: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 439us/sample - loss: 0.1458 - accuracy: 0.9514\n",
      "Epoch 470/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1438 - accuracy: 0.9517\n",
      "Epoch 00470: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 439us/sample - loss: 0.1452 - accuracy: 0.9470\n",
      "Epoch 471/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1383 - accuracy: 0.9432\n",
      "Epoch 00471: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 0.1448 - accuracy: 0.9470\n",
      "Epoch 472/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1501 - accuracy: 0.9460\n",
      "Epoch 00472: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 446us/sample - loss: 0.1438 - accuracy: 0.9514\n",
      "Epoch 473/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1369 - accuracy: 0.9545\n",
      "Epoch 00473: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 446us/sample - loss: 0.1460 - accuracy: 0.9492\n",
      "Epoch 474/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1513 - accuracy: 0.9460\n",
      "Epoch 00474: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 464us/sample - loss: 0.1440 - accuracy: 0.9492\n",
      "Epoch 475/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1397 - accuracy: 0.9432\n",
      "Epoch 00475: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 439us/sample - loss: 0.1432 - accuracy: 0.9448\n",
      "Epoch 476/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1409 - accuracy: 0.9517\n",
      "Epoch 00476: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 450us/sample - loss: 0.1425 - accuracy: 0.9514\n",
      "Epoch 477/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1532 - accuracy: 0.9489\n",
      "Epoch 00477: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 437us/sample - loss: 0.1418 - accuracy: 0.9492\n",
      "Epoch 478/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1634 - accuracy: 0.9375\n",
      "Epoch 00478: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 459us/sample - loss: 0.1415 - accuracy: 0.9470\n",
      "Epoch 479/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1375 - accuracy: 0.9460\n",
      "Epoch 00479: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 446us/sample - loss: 0.1413 - accuracy: 0.9448\n",
      "Epoch 480/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1489 - accuracy: 0.9375\n",
      "Epoch 00480: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 437us/sample - loss: 0.1410 - accuracy: 0.9426\n",
      "Epoch 481/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1473 - accuracy: 0.9403\n",
      "Epoch 00481: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 479us/sample - loss: 0.1404 - accuracy: 0.9470\n",
      "Epoch 482/500\n",
      "384/453 [========================>.....] - ETA: 0s - loss: 0.1528 - accuracy: 0.9427\n",
      "Epoch 00482: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 442us/sample - loss: 0.1406 - accuracy: 0.9470\n",
      "Epoch 483/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1410 - accuracy: 0.9432\n",
      "Epoch 00483: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 464us/sample - loss: 0.1399 - accuracy: 0.9448\n",
      "Epoch 484/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1445 - accuracy: 0.9517\n",
      "Epoch 00484: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 444us/sample - loss: 0.1390 - accuracy: 0.9536\n",
      "Epoch 485/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1461 - accuracy: 0.9489\n",
      "Epoch 00485: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 446us/sample - loss: 0.1382 - accuracy: 0.9514\n",
      "Epoch 486/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1450 - accuracy: 0.9432\n",
      "Epoch 00486: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 453us/sample - loss: 0.1381 - accuracy: 0.9470\n",
      "Epoch 487/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1260 - accuracy: 0.9545\n",
      "Epoch 00487: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 428us/sample - loss: 0.1382 - accuracy: 0.9448\n",
      "Epoch 488/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1366 - accuracy: 0.9545\n",
      "Epoch 00488: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 459us/sample - loss: 0.1387 - accuracy: 0.9514\n",
      "Epoch 489/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1586 - accuracy: 0.9347\n",
      "Epoch 00489: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 444us/sample - loss: 0.1378 - accuracy: 0.9470\n",
      "Epoch 490/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1433 - accuracy: 0.9489\n",
      "Epoch 00490: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 0.1374 - accuracy: 0.9492\n",
      "Epoch 491/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1404 - accuracy: 0.9460\n",
      "Epoch 00491: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 0.1365 - accuracy: 0.9470\n",
      "Epoch 492/500\n",
      "320/453 [====================>.........] - ETA: 0s - loss: 0.1546 - accuracy: 0.93 - ETA: 0s - loss: 0.1333 - accuracy: 0.9469\n",
      "Epoch 00492: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 472us/sample - loss: 0.1362 - accuracy: 0.9492\n",
      "Epoch 493/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1369 - accuracy: 0.9545\n",
      "Epoch 00493: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 446us/sample - loss: 0.1358 - accuracy: 0.9492\n",
      "Epoch 494/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1413 - accuracy: 0.9460\n",
      "Epoch 00494: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 457us/sample - loss: 0.1352 - accuracy: 0.9492\n",
      "Epoch 495/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1446 - accuracy: 0.9403\n",
      "Epoch 00495: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 459us/sample - loss: 0.1363 - accuracy: 0.9470\n",
      "Epoch 496/500\n",
      "384/453 [========================>.....] - ETA: 0s - loss: 0.1420 - accuracy: 0.9427\n",
      "Epoch 00496: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 448us/sample - loss: 0.1382 - accuracy: 0.9448\n",
      "Epoch 497/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1531 - accuracy: 0.9432\n",
      "Epoch 00497: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 455us/sample - loss: 0.1357 - accuracy: 0.9492\n",
      "Epoch 498/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1297 - accuracy: 0.9517\n",
      "Epoch 00498: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 461us/sample - loss: 0.1351 - accuracy: 0.9492\n",
      "Epoch 499/500\n",
      "352/453 [======================>.......] - ETA: 0s - loss: 0.1234 - accuracy: 0.9574\n",
      "Epoch 00499: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 439us/sample - loss: 0.1349 - accuracy: 0.9514\n",
      "Epoch 500/500\n",
      "384/453 [========================>.....] - ETA: 0s - loss: 0.1420 - accuracy: 0.9453\n",
      "Epoch 00500: saving model to data/text-Generation/\n",
      "453/453 [==============================] - 0s 450us/sample - loss: 0.1346 - accuracy: 0.9514\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(xs, ys, epochs=500, verbose=1, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1bec7cbf278>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(\"data/model-checkpoints/text-Generation/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'accuracy')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_poem(model, tokenizer, seed_text, next_words, reversed_word_index=None):\n",
    "\tif not reversed_word_index:\n",
    "\t\treversed_word_index = {value:key for key,value in tokenizer.word_index.items()}\n",
    "\n",
    "\tfor _ in range(next_words):\n",
    "\t\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\t\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "\t\tpredicted = model.predict_classes(token_list, verbose=0)\n",
    "\t\toutput_word = \"\"\n",
    "\t\ttry:\n",
    "\t\t\tseed_text += \" \" + reversed_word_index[predicted[0]]\n",
    "\t\texcept:\n",
    "\t\t\tpass\n",
    "\tprint(seed_text)\n",
    "\t\n",
    "\treturn reversed_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laurence went to dublin town of delight in the town of athy one town\n"
     ]
    }
   ],
   "source": [
    "reversed_word_index = create_poem(model, tokenizer, \"Laurence went to dublin\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the predictions dont seem great because after a few words it turns into jiberish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"data/songs/Laurences_generated_poetry.txt\").read().split(\"\\n\")\n",
    "data = list(map(str.strip, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys, max_sequence_len, total_words = preprocessor(tokenizer, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "num_epochs = 100\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(total_words, embedding_dim, input_length=max_sequence_len-1),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(150)),\n",
    "    tf.keras.layers.Dense(total_words, activation=\"softmax\")\n",
    "])\n",
    "adam_optimiser = tf.keras.optimizers.Adam(lr=0.01)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=adam_optimiser, metrics=[\"accuracy\"])\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"data/model-checkpoints/text-Generation2/\", save_weights_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12038 samples\n",
      "Epoch 1/100\n",
      "12032/12038 [============================>.] - ETA: 0s - loss: 6.6434 - accuracy: 0.0756\n",
      "Epoch 00001: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 7s 621us/sample - loss: 6.6428 - accuracy: 0.0756\n",
      "Epoch 2/100\n",
      "11936/12038 [============================>.] - ETA: 0s - loss: 5.7730 - accuracy: 0.1111\n",
      "Epoch 00002: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 371us/sample - loss: 5.7703 - accuracy: 0.1116\n",
      "Epoch 3/100\n",
      "12000/12038 [============================>.] - ETA: 0s - loss: 4.9331 - accuracy: 0.1608\n",
      "Epoch 00003: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 369us/sample - loss: 4.9328 - accuracy: 0.1610\n",
      "Epoch 4/100\n",
      "11936/12038 [============================>.] - ETA: 0s - loss: 4.0547 - accuracy: 0.2265 ETA: 0s - los\n",
      "Epoch 00004: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 405us/sample - loss: 4.0572 - accuracy: 0.2263\n",
      "Epoch 5/100\n",
      "11904/12038 [============================>.] - ETA: 0s - loss: 3.2381 - accuracy: 0.3279\n",
      "Epoch 00005: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 401us/sample - loss: 3.2453 - accuracy: 0.3267\n",
      "Epoch 6/100\n",
      "11936/12038 [============================>.] - ETA: 0s - loss: 2.6078 - accuracy: 0.4221\n",
      "Epoch 00006: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 405us/sample - loss: 2.6133 - accuracy: 0.4209\n",
      "Epoch 7/100\n",
      "12032/12038 [============================>.] - ETA: 0s - loss: 2.1507 - accuracy: 0.5057\n",
      "Epoch 00007: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 408us/sample - loss: 2.1511 - accuracy: 0.5056\n",
      "Epoch 8/100\n",
      "11872/12038 [============================>.] - ETA: 0s - loss: 1.7874 - accuracy: 0.5751\n",
      "Epoch 00008: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 375us/sample - loss: 1.7902 - accuracy: 0.5744\n",
      "Epoch 9/100\n",
      "12032/12038 [============================>.] - ETA: 0s - loss: 1.4907 - accuracy: 0.6431\n",
      "Epoch 00009: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 338us/sample - loss: 1.4907 - accuracy: 0.6430\n",
      "Epoch 10/100\n",
      "11904/12038 [============================>.] - ETA: 0s - loss: 1.3271 - accuracy: 0.6788\n",
      "Epoch 00010: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 368us/sample - loss: 1.3307 - accuracy: 0.6781\n",
      "Epoch 11/100\n",
      "12000/12038 [============================>.] - ETA: 0s - loss: 1.2186 - accuracy: 0.6996\n",
      "Epoch 00011: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 367us/sample - loss: 1.2212 - accuracy: 0.6991\n",
      "Epoch 12/100\n",
      "12032/12038 [============================>.] - ETA: 0s - loss: 1.1119 - accuracy: 0.7233\n",
      "Epoch 00012: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 399us/sample - loss: 1.1120 - accuracy: 0.7233\n",
      "Epoch 13/100\n",
      "12000/12038 [============================>.] - ETA: 0s - loss: 1.1026 - accuracy: 0.7209\n",
      "Epoch 00013: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 395us/sample - loss: 1.1038 - accuracy: 0.7209\n",
      "Epoch 14/100\n",
      "12032/12038 [============================>.] - ETA: 0s - loss: 1.1189 - accuracy: 0.7151\n",
      "Epoch 00014: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 390us/sample - loss: 1.1192 - accuracy: 0.7151\n",
      "Epoch 15/100\n",
      "11968/12038 [============================>.] - ETA: 0s - loss: 1.1489 - accuracy: 0.7048\n",
      "Epoch 00015: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 394us/sample - loss: 1.1525 - accuracy: 0.7041\n",
      "Epoch 16/100\n",
      "11936/12038 [============================>.] - ETA: 0s - loss: 1.0750 - accuracy: 0.7236\n",
      "Epoch 00016: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 398us/sample - loss: 1.0741 - accuracy: 0.7239\n",
      "Epoch 17/100\n",
      "12032/12038 [============================>.] - ETA: 0s - loss: 1.0162 - accuracy: 0.7400\n",
      "Epoch 00017: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 397us/sample - loss: 1.0163 - accuracy: 0.7399\n",
      "Epoch 18/100\n",
      "12000/12038 [============================>.] - ETA: 0s - loss: 1.0051 - accuracy: 0.7415\n",
      "Epoch 00018: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 392us/sample - loss: 1.0065 - accuracy: 0.7412\n",
      "Epoch 19/100\n",
      "11936/12038 [============================>.] - ETA: 0s - loss: 1.0184 - accuracy: 0.7325\n",
      "Epoch 00019: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 379us/sample - loss: 1.0239 - accuracy: 0.7314\n",
      "Epoch 20/100\n",
      "11968/12038 [============================>.] - ETA: 0s - loss: 0.9987 - accuracy: 0.7370\n",
      "Epoch 00020: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 405us/sample - loss: 1.0016 - accuracy: 0.7362\n",
      "Epoch 21/100\n",
      "11872/12038 [============================>.] - ETA: 0s - loss: 1.0143 - accuracy: 0.7315\n",
      "Epoch 00021: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 375us/sample - loss: 1.0170 - accuracy: 0.7312\n",
      "Epoch 22/100\n",
      "12000/12038 [============================>.] - ETA: 0s - loss: 0.9805 - accuracy: 0.7398\n",
      "Epoch 00022: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 3s 274us/sample - loss: 0.9824 - accuracy: 0.7397\n",
      "Epoch 23/100\n",
      "11904/12038 [============================>.] - ETA: 0s - loss: 1.0100 - accuracy: 0.7372\n",
      "Epoch 00023: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 331us/sample - loss: 1.0136 - accuracy: 0.7363\n",
      "Epoch 24/100\n",
      "11968/12038 [============================>.] - ETA: 0s - loss: 1.0091 - accuracy: 0.7324\n",
      "Epoch 00024: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 362us/sample - loss: 1.0125 - accuracy: 0.7317\n",
      "Epoch 25/100\n",
      "11904/12038 [============================>.] - ETA: 0s - loss: 1.0266 - accuracy: 0.7342\n",
      "Epoch 00025: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 347us/sample - loss: 1.0302 - accuracy: 0.7338\n",
      "Epoch 26/100\n",
      "12032/12038 [============================>.] - ETA: 0s - loss: 1.0296 - accuracy: 0.7291\n",
      "Epoch 00026: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 373us/sample - loss: 1.0296 - accuracy: 0.7290\n",
      "Epoch 27/100\n",
      "11936/12038 [============================>.] - ETA: 0s - loss: 0.9667 - accuracy: 0.7450\n",
      "Epoch 00027: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 363us/sample - loss: 0.9688 - accuracy: 0.7444\n",
      "Epoch 28/100\n",
      "11904/12038 [============================>.] - ETA: 0s - loss: 0.9191 - accuracy: 0.7559\n",
      "Epoch 00028: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 364us/sample - loss: 0.9227 - accuracy: 0.7549\n",
      "Epoch 29/100\n",
      "12032/12038 [============================>.] - ETA: 0s - loss: 0.8900 - accuracy: 0.7615\n",
      "Epoch 00029: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 366us/sample - loss: 0.8910 - accuracy: 0.7612\n",
      "Epoch 30/100\n",
      "11968/12038 [============================>.] - ETA: 0s - loss: 0.8758 - accuracy: 0.7701\n",
      "Epoch 00030: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 359us/sample - loss: 0.8765 - accuracy: 0.7701\n",
      "Epoch 31/100\n",
      "11936/12038 [============================>.] - ETA: 0s - loss: 0.9109 - accuracy: 0.7591\n",
      "Epoch 00031: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 368us/sample - loss: 0.9126 - accuracy: 0.7586\n",
      "Epoch 32/100\n",
      "11968/12038 [============================>.] - ETA: 0s - loss: 0.9922 - accuracy: 0.7400\n",
      "Epoch 00032: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 370us/sample - loss: 0.9920 - accuracy: 0.7401\n",
      "Epoch 33/100\n",
      "11968/12038 [============================>.] - ETA: 0s - loss: 0.9807 - accuracy: 0.7423\n",
      "Epoch 00033: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 369us/sample - loss: 0.9801 - accuracy: 0.7423\n",
      "Epoch 34/100\n",
      "11936/12038 [============================>.] - ETA: 0s - loss: 0.9418 - accuracy: 0.7502\n",
      "Epoch 00034: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 360us/sample - loss: 0.9441 - accuracy: 0.7498\n",
      "Epoch 35/100\n",
      "12032/12038 [============================>.] - ETA: 0s - loss: 0.9477 - accuracy: 0.7482\n",
      "Epoch 00035: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 369us/sample - loss: 0.9474 - accuracy: 0.7482\n",
      "Epoch 36/100\n",
      "11904/12038 [============================>.] - ETA: 0s - loss: 0.9613 - accuracy: 0.7431\n",
      "Epoch 00036: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 360us/sample - loss: 0.9658 - accuracy: 0.7422\n",
      "Epoch 37/100\n",
      "11968/12038 [============================>.] - ETA: 0s - loss: 0.9297 - accuracy: 0.7515\n",
      "Epoch 00037: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 376us/sample - loss: 0.9316 - accuracy: 0.7511\n",
      "Epoch 38/100\n",
      "11936/12038 [============================>.] - ETA: 0s - loss: 0.9803 - accuracy: 0.7458\n",
      "Epoch 00038: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 366us/sample - loss: 0.9792 - accuracy: 0.7461\n",
      "Epoch 39/100\n",
      "11968/12038 [============================>.] - ETA: 0s - loss: 1.0175 - accuracy: 0.7321\n",
      "Epoch 00039: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 368us/sample - loss: 1.0192 - accuracy: 0.7316\n",
      "Epoch 40/100\n",
      "11936/12038 [============================>.] - ETA: 0s - loss: 1.0079 - accuracy: 0.7350\n",
      "Epoch 00040: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 361us/sample - loss: 1.0083 - accuracy: 0.7352\n",
      "Epoch 41/100\n",
      "11936/12038 [============================>.] - ETA: 0s - loss: 0.9273 - accuracy: 0.7555\n",
      "Epoch 00041: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 366us/sample - loss: 0.9290 - accuracy: 0.7554\n",
      "Epoch 42/100\n",
      "11968/12038 [============================>.] - ETA: 0s - loss: 0.8649 - accuracy: 0.7680\n",
      "Epoch 00042: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 364us/sample - loss: 0.8647 - accuracy: 0.7681\n",
      "Epoch 43/100\n",
      "12000/12038 [============================>.] - ETA: 0s - loss: 0.8497 - accuracy: 0.7757\n",
      "Epoch 00043: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 341us/sample - loss: 0.8524 - accuracy: 0.7753\n",
      "Epoch 44/100\n",
      "11904/12038 [============================>.] - ETA: 0s - loss: 0.8292 - accuracy: 0.7784\n",
      "Epoch 00044: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 350us/sample - loss: 0.8311 - accuracy: 0.7782\n",
      "Epoch 45/100\n",
      "12032/12038 [============================>.] - ETA: 0s - loss: 0.8451 - accuracy: 0.7776\n",
      "Epoch 00045: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 357us/sample - loss: 0.8450 - accuracy: 0.7776\n",
      "Epoch 46/100\n",
      "12000/12038 [============================>.] - ETA: 0s - loss: 0.8583 - accuracy: 0.7682\n",
      "Epoch 00046: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 376us/sample - loss: 0.8583 - accuracy: 0.7683\n",
      "Epoch 47/100\n",
      "12032/12038 [============================>.] - ETA: 0s - loss: 0.9326 - accuracy: 0.7518\n",
      "Epoch 00047: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 363us/sample - loss: 0.9324 - accuracy: 0.7519\n",
      "Epoch 48/100\n",
      "12000/12038 [============================>.] - ETA: 0s - loss: 1.0928 - accuracy: 0.7157\n",
      "Epoch 00048: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 359us/sample - loss: 1.0933 - accuracy: 0.7156\n",
      "Epoch 49/100\n",
      "11936/12038 [============================>.] - ETA: 0s - loss: 1.0746 - accuracy: 0.7169\n",
      "Epoch 00049: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 364us/sample - loss: 1.0764 - accuracy: 0.7163\n",
      "Epoch 50/100\n",
      "12000/12038 [============================>.] - ETA: 0s - loss: 1.0554 - accuracy: 0.7236\n",
      "Epoch 00050: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 351us/sample - loss: 1.0560 - accuracy: 0.7234\n",
      "Epoch 51/100\n",
      "11936/12038 [============================>.] - ETA: 0s - loss: 1.0051 - accuracy: 0.7342\n",
      "Epoch 00051: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 368us/sample - loss: 1.0065 - accuracy: 0.7339\n",
      "Epoch 52/100\n",
      "12032/12038 [============================>.] - ETA: 0s - loss: 0.9605 - accuracy: 0.7448\n",
      "Epoch 00052: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 355us/sample - loss: 0.9612 - accuracy: 0.7446\n",
      "Epoch 53/100\n",
      "11968/12038 [============================>.] - ETA: 0s - loss: 0.8937 - accuracy: 0.7593\n",
      "Epoch 00053: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 310us/sample - loss: 0.8965 - accuracy: 0.7586\n",
      "Epoch 54/100\n",
      "11968/12038 [============================>.] - ETA: 0s - loss: 0.8131 - accuracy: 0.7800\n",
      "Epoch 00054: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 324us/sample - loss: 0.8163 - accuracy: 0.7794\n",
      "Epoch 55/100\n",
      "12032/12038 [============================>.] - ETA: 0s - loss: 0.7734 - accuracy: 0.7948\n",
      "Epoch 00055: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 306us/sample - loss: 0.7740 - accuracy: 0.7947\n",
      "Epoch 56/100\n",
      "11968/12038 [============================>.] - ETA: 0s - loss: 0.7406 - accuracy: 0.8021\n",
      "Epoch 00056: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 368us/sample - loss: 0.7415 - accuracy: 0.8020\n",
      "Epoch 57/100\n",
      "11936/12038 [============================>.] - ETA: 0s - loss: 0.8143 - accuracy: 0.7840\n",
      "Epoch 00057: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 361us/sample - loss: 0.8152 - accuracy: 0.7835\n",
      "Epoch 58/100\n",
      "12000/12038 [============================>.] - ETA: 0s - loss: 0.9945 - accuracy: 0.7387\n",
      "Epoch 00058: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 375us/sample - loss: 0.9939 - accuracy: 0.7388\n",
      "Epoch 59/100\n",
      "12000/12038 [============================>.] - ETA: 0s - loss: 1.2083 - accuracy: 0.6953\n",
      "Epoch 00059: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 365us/sample - loss: 1.2074 - accuracy: 0.6955\n",
      "Epoch 60/100\n",
      "12000/12038 [============================>.] - ETA: 0s - loss: 1.1730 - accuracy: 0.7007\n",
      "Epoch 00060: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 365us/sample - loss: 1.1725 - accuracy: 0.7008\n",
      "Epoch 61/100\n",
      "11904/12038 [============================>.] - ETA: 0s - loss: 1.0813 - accuracy: 0.7228\n",
      "Epoch 00061: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 361us/sample - loss: 1.0824 - accuracy: 0.7227\n",
      "Epoch 62/100\n",
      "11904/12038 [============================>.] - ETA: 0s - loss: 0.9559 - accuracy: 0.7515\n",
      "Epoch 00062: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 368us/sample - loss: 0.9560 - accuracy: 0.7515\n",
      "Epoch 63/100\n",
      "11936/12038 [============================>.] - ETA: 0s - loss: 0.9007 - accuracy: 0.7613\n",
      "Epoch 00063: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 355us/sample - loss: 0.9034 - accuracy: 0.7608\n",
      "Epoch 64/100\n",
      "11904/12038 [============================>.] - ETA: 0s - loss: 0.8303 - accuracy: 0.7814\n",
      "Epoch 00064: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 369us/sample - loss: 0.8334 - accuracy: 0.7806\n",
      "Epoch 65/100\n",
      "12032/12038 [============================>.] - ETA: 0s - loss: 0.7863 - accuracy: 0.7931\n",
      "Epoch 00065: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 358us/sample - loss: 0.7863 - accuracy: 0.7931\n",
      "Epoch 66/100\n",
      "11936/12038 [============================>.] - ETA: 0s - loss: 0.7940 - accuracy: 0.7912\n",
      "Epoch 00066: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 362us/sample - loss: 0.7938 - accuracy: 0.7911\n",
      "Epoch 67/100\n",
      "11936/12038 [============================>.] - ETA: 0s - loss: 0.8011 - accuracy: 0.7837\n",
      "Epoch 00067: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 365us/sample - loss: 0.8067 - accuracy: 0.7826\n",
      "Epoch 68/100\n",
      "12000/12038 [============================>.] - ETA: 0s - loss: 0.8610 - accuracy: 0.7726\n",
      "Epoch 00068: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 377us/sample - loss: 0.8613 - accuracy: 0.7722\n",
      "Epoch 69/100\n",
      "11968/12038 [============================>.] - ETA: 0s - loss: 0.9083 - accuracy: 0.7607\n",
      "Epoch 00069: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 366us/sample - loss: 0.9089 - accuracy: 0.7606\n",
      "Epoch 70/100\n",
      "12000/12038 [============================>.] - ETA: 0s - loss: 0.9593 - accuracy: 0.7463\n",
      "Epoch 00070: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 368us/sample - loss: 0.9610 - accuracy: 0.7461\n",
      "Epoch 71/100\n",
      "11968/12038 [============================>.] - ETA: 0s - loss: 1.0370 - accuracy: 0.7285\n",
      "Epoch 00071: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 375us/sample - loss: 1.0351 - accuracy: 0.7289\n",
      "Epoch 72/100\n",
      "12000/12038 [============================>.] - ETA: 0s - loss: 1.0662 - accuracy: 0.7212\n",
      "Epoch 00072: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 371us/sample - loss: 1.0657 - accuracy: 0.7215\n",
      "Epoch 73/100\n",
      "12000/12038 [============================>.] - ETA: 0s - loss: 0.9302 - accuracy: 0.7556\n",
      "Epoch 00073: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 371us/sample - loss: 0.9309 - accuracy: 0.7554\n",
      "Epoch 74/100\n",
      "12032/12038 [============================>.] - ETA: 0s - loss: 0.9110 - accuracy: 0.7607\n",
      "Epoch 00074: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 371us/sample - loss: 0.9114 - accuracy: 0.7606\n",
      "Epoch 75/100\n",
      "11936/12038 [============================>.] - ETA: 0s - loss: 0.8552 - accuracy: 0.7717\n",
      "Epoch 00075: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 374us/sample - loss: 0.8546 - accuracy: 0.7721\n",
      "Epoch 76/100\n",
      "11968/12038 [============================>.] - ETA: 0s - loss: 0.8147 - accuracy: 0.7830\n",
      "Epoch 00076: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 371us/sample - loss: 0.8176 - accuracy: 0.7823\n",
      "Epoch 77/100\n",
      "11936/12038 [============================>.] - ETA: 0s - loss: 0.8142 - accuracy: 0.7813\n",
      "Epoch 00077: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 379us/sample - loss: 0.8154 - accuracy: 0.7811\n",
      "Epoch 78/100\n",
      "11936/12038 [============================>.] - ETA: 0s - loss: 0.8272 - accuracy: 0.7812\n",
      "Epoch 00078: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 370us/sample - loss: 0.8282 - accuracy: 0.7809\n",
      "Epoch 79/100\n",
      "11936/12038 [============================>.] - ETA: 0s - loss: 0.8527 - accuracy: 0.7745\n",
      "Epoch 00079: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 384us/sample - loss: 0.8523 - accuracy: 0.7748\n",
      "Epoch 80/100\n",
      "11904/12038 [============================>.] - ETA: 0s - loss: 0.9033 - accuracy: 0.7651\n",
      "Epoch 00080: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 374us/sample - loss: 0.9059 - accuracy: 0.7648\n",
      "Epoch 81/100\n",
      "11968/12038 [============================>.] - ETA: 0s - loss: 0.9477 - accuracy: 0.7520\n",
      "Epoch 00081: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 376us/sample - loss: 0.9476 - accuracy: 0.7519\n",
      "Epoch 82/100\n",
      "11936/12038 [============================>.] - ETA: 0s - loss: 0.9749 - accuracy: 0.7463 E\n",
      "Epoch 00082: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 384us/sample - loss: 0.9758 - accuracy: 0.7459\n",
      "Epoch 83/100\n",
      "12032/12038 [============================>.] - ETA: 0s - loss: 0.9376 - accuracy: 0.7537\n",
      "Epoch 00083: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 384us/sample - loss: 0.9381 - accuracy: 0.7536\n",
      "Epoch 84/100\n",
      "12000/12038 [============================>.] - ETA: 0s - loss: 0.9695 - accuracy: 0.7466\n",
      "Epoch 00084: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 379us/sample - loss: 0.9676 - accuracy: 0.7471\n",
      "Epoch 85/100\n",
      "11968/12038 [============================>.] - ETA: 0s - loss: 0.9173 - accuracy: 0.7599\n",
      "Epoch 00085: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 376us/sample - loss: 0.9190 - accuracy: 0.7597\n",
      "Epoch 86/100\n",
      "12032/12038 [============================>.] - ETA: 0s - loss: 0.8865 - accuracy: 0.7660\n",
      "Epoch 00086: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 378us/sample - loss: 0.8864 - accuracy: 0.7660\n",
      "Epoch 87/100\n",
      "11936/12038 [============================>.] - ETA: 0s - loss: 0.8484 - accuracy: 0.7776\n",
      "Epoch 00087: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 376us/sample - loss: 0.8490 - accuracy: 0.7779\n",
      "Epoch 88/100\n",
      "12032/12038 [============================>.] - ETA: 0s - loss: 0.8203 - accuracy: 0.7794\n",
      "Epoch 00088: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 381us/sample - loss: 0.8205 - accuracy: 0.7794\n",
      "Epoch 89/100\n",
      "11904/12038 [============================>.] - ETA: 0s - loss: 0.8336 - accuracy: 0.7814\n",
      "Epoch 00089: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 370us/sample - loss: 0.8369 - accuracy: 0.7812\n",
      "Epoch 90/100\n",
      "12032/12038 [============================>.] - ETA: 0s - loss: 0.8385 - accuracy: 0.7768\n",
      "Epoch 00090: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 376us/sample - loss: 0.8385 - accuracy: 0.7767\n",
      "Epoch 91/100\n",
      "12000/12038 [============================>.] - ETA: 0s - loss: 0.9391 - accuracy: 0.7602\n",
      "Epoch 00091: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 373us/sample - loss: 0.9391 - accuracy: 0.7601\n",
      "Epoch 92/100\n",
      "12000/12038 [============================>.] - ETA: 0s - loss: 0.9090 - accuracy: 0.7638\n",
      "Epoch 00092: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 377us/sample - loss: 0.9099 - accuracy: 0.7634\n",
      "Epoch 93/100\n",
      "11936/12038 [============================>.] - ETA: 0s - loss: 0.9184 - accuracy: 0.7606\n",
      "Epoch 00093: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 374us/sample - loss: 0.9207 - accuracy: 0.7598\n",
      "Epoch 94/100\n",
      "11904/12038 [============================>.] - ETA: 0s - loss: 0.8907 - accuracy: 0.7658\n",
      "Epoch 00094: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 368us/sample - loss: 0.8942 - accuracy: 0.7648\n",
      "Epoch 95/100\n",
      "12000/12038 [============================>.] - ETA: 0s - loss: 0.8661 - accuracy: 0.7747\n",
      "Epoch 00095: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 370us/sample - loss: 0.8660 - accuracy: 0.7746\n",
      "Epoch 96/100\n",
      "11968/12038 [============================>.] - ETA: 0s - loss: 0.8486 - accuracy: 0.7797\n",
      "Epoch 00096: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 373us/sample - loss: 0.8496 - accuracy: 0.7798\n",
      "Epoch 97/100\n",
      "11968/12038 [============================>.] - ETA: 0s - loss: 0.8306 - accuracy: 0.7838\n",
      "Epoch 00097: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 387us/sample - loss: 0.8328 - accuracy: 0.7830\n",
      "Epoch 98/100\n",
      "11968/12038 [============================>.] - ETA: 0s - loss: 0.8866 - accuracy: 0.7687\n",
      "Epoch 00098: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 376us/sample - loss: 0.8854 - accuracy: 0.7686\n",
      "Epoch 99/100\n",
      "12032/12038 [============================>.] - ETA: 0s - loss: 0.8701 - accuracy: 0.7737\n",
      "Epoch 00099: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 5s 378us/sample - loss: 0.8704 - accuracy: 0.7736\n",
      "Epoch 100/100\n",
      "12000/12038 [============================>.] - ETA: 0s - loss: 0.8583 - accuracy: 0.7744\n",
      "Epoch 00100: saving model to data/model-checkpoints/text-Generation2/\n",
      "12038/12038 [==============================] - 4s 369us/sample - loss: 0.8591 - accuracy: 0.7744\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(xs, ys, epochs=num_epochs, verbose=1, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'accuracy')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laurence went to dublin town of delight in the town of athy one town of green around ballyjamesduff fashion free academy squall till first my room for erin go bragh i head so lead i oft times did more those bright as until you brother me who did will hear from below come easy on each breeze there will there we pepper on kilgary mountain near being like wine for else old skibbereen isle they were hearty heart she and fair maiden clearly my love in fray or gown loud you able to a huff of dungannon new both it was a trusty tree\n"
     ]
    }
   ],
   "source": [
    "reversed_word_index = create_poem(model, tokenizer, \"Laurence went to dublin\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When shall also try to train the model with tokenised characters instead of words. Shocking to know that the model can actually learn how to spell words and put together meaningful words. It does fail at grammatical structures however. I have a feeling CNNs would be useful for that but that's just speculation.\n",
    "# Char-level NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"data/songs/Laurences_generated_poetry.txt\").read().split(\"\\n\")\n",
    "data = list(map(str.strip, data))\n",
    "text = ' '.join(data) # make sure your corpus has at least ~100k characters. ~1M is better\n",
    "# len(text) = 68,998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet=\"\".join([chr(i) for i in range(32,127)])\n",
    "chartokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True) # oov_token=\"<OOV>\"\n",
    "chartokenizer.fit_on_texts(text+alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 20\n",
    "step = 3\n",
    "total_chars = len(chartokenizer.word_index)\n",
    "embedding_dim = 168\n",
    "num_epochs = 100\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "input_chars = []\n",
    "next_char = []\n",
    "for i in range(0, len(text) - sequence_length, step):\n",
    "    input_chars.append(text[i : i + sequence_length])\n",
    "    next_char.append(text[i + sequence_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 22993\n",
      "input X  (input_chars)  --->   output y (next_char) \n",
      "Come all ye maidens     --->   y\n",
      "e all ye maidens you    --->   n\n",
      "ll ye maidens young     --->   a\n",
      "ye maidens young and    --->    \n",
      "maidens young and fa    --->   i\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of sequences:\", len(input_chars))\n",
    "print(\"input X  (input_chars)  --->   output y (next_char) \")\n",
    "for i in range(5):\n",
    "  print( input_chars[i],\"   --->  \", next_char[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = chartokenizer.texts_to_sequences(input_chars)\n",
    "labels = chartokenizer.texts_to_sequences(next_char)\n",
    "labels = list(map(lambda lst: lst[0], labels))\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=total_chars) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(total_chars, embedding_dim, input_length=sequence_length),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(150)),\n",
    "    tf.keras.layers.Dense(total_chars, activation=\"softmax\")\n",
    "])\n",
    "adam_optimiser = tf.keras.optimizers.Adam(lr=0.01)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=adam_optimiser, metrics=[\"accuracy\"])\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"data/model-checkpoints/text-Generation_char_lvl/\", save_weights_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22993 samples\n",
      "Epoch 1/100\n",
      "22848/22993 [============================>.] - ETA: 0s - loss: 2.1603 - accuracy: 0.3669\n",
      "Epoch 00001: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 11s 465us/sample - loss: 2.1596 - accuracy: 0.3672\n",
      "Epoch 2/100\n",
      "22976/22993 [============================>.] - ETA: 0s - loss: 1.8880 - accuracy: 0.4265\n",
      "Epoch 00002: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 335us/sample - loss: 1.8878 - accuracy: 0.4266\n",
      "Epoch 3/100\n",
      "22880/22993 [============================>.] - ETA: 0s - loss: 1.8142 - accuracy: 0.4432\n",
      "Epoch 00003: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 329us/sample - loss: 1.8151 - accuracy: 0.4431\n",
      "Epoch 4/100\n",
      "22848/22993 [============================>.] - ETA: 0s - loss: 1.7645 - accuracy: 0.4543\n",
      "Epoch 00004: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 332us/sample - loss: 1.7649 - accuracy: 0.4543\n",
      "Epoch 5/100\n",
      "22848/22993 [============================>.] - ETA: 0s - loss: 1.7163 - accuracy: 0.4659\n",
      "Epoch 00005: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 7s 324us/sample - loss: 1.7170 - accuracy: 0.4656\n",
      "Epoch 6/100\n",
      "22880/22993 [============================>.] - ETA: 0s - loss: 1.6975 - accuracy: 0.4767\n",
      "Epoch 00006: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 328us/sample - loss: 1.6980 - accuracy: 0.4766\n",
      "Epoch 7/100\n",
      "22880/22993 [============================>.] - ETA: 0s - loss: 1.6629 - accuracy: 0.4827\n",
      "Epoch 00007: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 328us/sample - loss: 1.6640 - accuracy: 0.4822\n",
      "Epoch 8/100\n",
      "22880/22993 [============================>.] - ETA: 0s - loss: 1.6390 - accuracy: 0.4847\n",
      "Epoch 00008: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 334us/sample - loss: 1.6402 - accuracy: 0.4844\n",
      "Epoch 9/100\n",
      "22976/22993 [============================>.] - ETA: 0s - loss: 1.6159 - accuracy: 0.4927\n",
      "Epoch 00009: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 337us/sample - loss: 1.6160 - accuracy: 0.4928\n",
      "Epoch 10/100\n",
      "22944/22993 [============================>.] - ETA: 0s - loss: 1.6041 - accuracy: 0.4952\n",
      "Epoch 00010: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 332us/sample - loss: 1.6036 - accuracy: 0.4952\n",
      "Epoch 11/100\n",
      "22880/22993 [============================>.] - ETA: 0s - loss: 1.5824 - accuracy: 0.5016\n",
      "Epoch 00011: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 343us/sample - loss: 1.5825 - accuracy: 0.5016\n",
      "Epoch 12/100\n",
      "22944/22993 [============================>.] - ETA: 0s - loss: 1.5710 - accuracy: 0.5051\n",
      "Epoch 00012: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 344us/sample - loss: 1.5710 - accuracy: 0.5051\n",
      "Epoch 13/100\n",
      "22912/22993 [============================>.] - ETA: 0s - loss: 1.5639 - accuracy: 0.5069\n",
      "Epoch 00013: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 332us/sample - loss: 1.5641 - accuracy: 0.5070\n",
      "Epoch 14/100\n",
      "22848/22993 [============================>.] - ETA: 0s - loss: 1.5592 - accuracy: 0.5041\n",
      "Epoch 00014: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 337us/sample - loss: 1.5601 - accuracy: 0.5038\n",
      "Epoch 15/100\n",
      "22912/22993 [============================>.] - ETA: 0s - loss: 1.5519 - accuracy: 0.5046\n",
      "Epoch 00015: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 335us/sample - loss: 1.5519 - accuracy: 0.5045\n",
      "Epoch 16/100\n",
      "22944/22993 [============================>.] - ETA: 0s - loss: 1.5514 - accuracy: 0.5060\n",
      "Epoch 00016: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 7s 321us/sample - loss: 1.5512 - accuracy: 0.5061\n",
      "Epoch 17/100\n",
      "22848/22993 [============================>.] - ETA: 0s - loss: 1.5528 - accuracy: 0.5063\n",
      "Epoch 00017: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 341us/sample - loss: 1.5533 - accuracy: 0.5063\n",
      "Epoch 18/100\n",
      "22848/22993 [============================>.] - ETA: 0s - loss: 1.5326 - accuracy: 0.5156\n",
      "Epoch 00018: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 7s 320us/sample - loss: 1.5318 - accuracy: 0.5159\n",
      "Epoch 19/100\n",
      "22880/22993 [============================>.] - ETA: 0s - loss: 1.5292 - accuracy: 0.5134\n",
      "Epoch 00019: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 338us/sample - loss: 1.5290 - accuracy: 0.5132\n",
      "Epoch 20/100\n",
      "22944/22993 [============================>.] - ETA: 0s - loss: 1.5320 - accuracy: 0.5131\n",
      "Epoch 00020: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 346us/sample - loss: 1.5322 - accuracy: 0.5130\n",
      "Epoch 21/100\n",
      "22944/22993 [============================>.] - ETA: 0s - loss: 1.5156 - accuracy: 0.5139\n",
      "Epoch 00021: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 338us/sample - loss: 1.5158 - accuracy: 0.5139\n",
      "Epoch 22/100\n",
      "22976/22993 [============================>.] - ETA: 0s - loss: 1.5358 - accuracy: 0.5152\n",
      "Epoch 00022: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 337us/sample - loss: 1.5359 - accuracy: 0.5153\n",
      "Epoch 23/100\n",
      "22912/22993 [============================>.] - ETA: 0s - loss: 1.5254 - accuracy: 0.5129\n",
      "Epoch 00023: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 353us/sample - loss: 1.5257 - accuracy: 0.5128\n",
      "Epoch 24/100\n",
      "22880/22993 [============================>.] - ETA: 0s - loss: 1.5158 - accuracy: 0.5198\n",
      "Epoch 00024: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 9s 390us/sample - loss: 1.5162 - accuracy: 0.5198\n",
      "Epoch 25/100\n",
      "22976/22993 [============================>.] - ETA: 0s - loss: 1.5148 - accuracy: 0.5164\n",
      "Epoch 00025: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 9s 377us/sample - loss: 1.5153 - accuracy: 0.5163\n",
      "Epoch 26/100\n",
      "22944/22993 [============================>.] - ETA: 0s - loss: 1.5165 - accuracy: 0.5183\n",
      "Epoch 00026: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 9s 379us/sample - loss: 1.5161 - accuracy: 0.5185\n",
      "Epoch 27/100\n",
      "22976/22993 [============================>.] - ETA: 0s - loss: 1.4988 - accuracy: 0.5203\n",
      "Epoch 00027: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 368us/sample - loss: 1.4988 - accuracy: 0.5202\n",
      "Epoch 28/100\n",
      "22944/22993 [============================>.] - ETA: 0s - loss: 1.5056 - accuracy: 0.5179\n",
      "Epoch 00028: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 9s 372us/sample - loss: 1.5063 - accuracy: 0.5177\n",
      "Epoch 29/100\n",
      "22880/22993 [============================>.] - ETA: 0s - loss: 1.5232 - accuracy: 0.5107\n",
      "Epoch 00029: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 368us/sample - loss: 1.5239 - accuracy: 0.5103\n",
      "Epoch 30/100\n",
      "22912/22993 [============================>.] - ETA: 0s - loss: 1.5061 - accuracy: 0.5207\n",
      "Epoch 00030: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 367us/sample - loss: 1.5064 - accuracy: 0.5207\n",
      "Epoch 31/100\n",
      "22976/22993 [============================>.] - ETA: 0s - loss: 1.5185 - accuracy: 0.5148 ETA: 0s\n",
      "Epoch 00031: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 9s 382us/sample - loss: 1.5189 - accuracy: 0.5147\n",
      "Epoch 32/100\n",
      "22976/22993 [============================>.] - ETA: 0s - loss: 1.5285 - accuracy: 0.5126\n",
      "Epoch 00032: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 9s 381us/sample - loss: 1.5286 - accuracy: 0.5127\n",
      "Epoch 33/100\n",
      "22944/22993 [============================>.] - ETA: 0s - loss: 1.5554 - accuracy: 0.5083\n",
      "Epoch 00033: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 9s 387us/sample - loss: 1.5556 - accuracy: 0.5082\n",
      "Epoch 34/100\n",
      "22944/22993 [============================>.] - ETA: 0s - loss: 1.5211 - accuracy: 0.5156\n",
      "Epoch 00034: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 9s 379us/sample - loss: 1.5206 - accuracy: 0.5157\n",
      "Epoch 35/100\n",
      "22944/22993 [============================>.] - ETA: 0s - loss: 1.5413 - accuracy: 0.5098\n",
      "Epoch 00035: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 9s 374us/sample - loss: 1.5424 - accuracy: 0.5095\n",
      "Epoch 36/100\n",
      "22976/22993 [============================>.] - ETA: 0s - loss: 1.5335 - accuracy: 0.5097\n",
      "Epoch 00036: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 363us/sample - loss: 1.5333 - accuracy: 0.5097\n",
      "Epoch 37/100\n",
      "22912/22993 [============================>.] - ETA: 0s - loss: 1.5354 - accuracy: 0.5095 ETA: 0s - l\n",
      "Epoch 00037: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 9s 372us/sample - loss: 1.5352 - accuracy: 0.5095\n",
      "Epoch 38/100\n",
      "22848/22993 [============================>.] - ETA: 0s - loss: 1.5262 - accuracy: 0.5133\n",
      "Epoch 00038: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 9s 377us/sample - loss: 1.5269 - accuracy: 0.5128\n",
      "Epoch 39/100\n",
      "22848/22993 [============================>.] - ETA: 0s - loss: 1.5280 - accuracy: 0.5111\n",
      "Epoch 00039: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 366us/sample - loss: 1.5269 - accuracy: 0.5114\n",
      "Epoch 40/100\n",
      "22944/22993 [============================>.] - ETA: 0s - loss: 1.5331 - accuracy: 0.5119\n",
      "Epoch 00040: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 9s 384us/sample - loss: 1.5331 - accuracy: 0.5117\n",
      "Epoch 41/100\n",
      "22848/22993 [============================>.] - ETA: 0s - loss: 1.5741 - accuracy: 0.5062 ETA: 0s\n",
      "Epoch 00041: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 338us/sample - loss: 1.5732 - accuracy: 0.5066\n",
      "Epoch 42/100\n",
      "22848/22993 [============================>.] - ETA: 0s - loss: 1.5589 - accuracy: 0.5049\n",
      "Epoch 00042: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 354us/sample - loss: 1.5588 - accuracy: 0.5052\n",
      "Epoch 43/100\n",
      "22880/22993 [============================>.] - ETA: 0s - loss: 1.5850 - accuracy: 0.4999\n",
      "Epoch 00043: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 9s 375us/sample - loss: 1.5852 - accuracy: 0.4999\n",
      "Epoch 44/100\n",
      "22880/22993 [============================>.] - ETA: 0s - loss: 1.6033 - accuracy: 0.4968\n",
      "Epoch 00044: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 9s 373us/sample - loss: 1.6049 - accuracy: 0.4965\n",
      "Epoch 45/100\n",
      "22848/22993 [============================>.] - ETA: 0s - loss: 1.5811 - accuracy: 0.4997\n",
      "Epoch 00045: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 9s 376us/sample - loss: 1.5807 - accuracy: 0.4999\n",
      "Epoch 46/100\n",
      "22912/22993 [============================>.] - ETA: 0s - loss: 1.5914 - accuracy: 0.4960\n",
      "Epoch 00046: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 352us/sample - loss: 1.5921 - accuracy: 0.4958\n",
      "Epoch 47/100\n",
      "22944/22993 [============================>.] - ETA: 0s - loss: 1.5783 - accuracy: 0.5048\n",
      "Epoch 00047: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 337us/sample - loss: 1.5786 - accuracy: 0.5047\n",
      "Epoch 48/100\n",
      "22944/22993 [============================>.] - ETA: 0s - loss: 1.5564 - accuracy: 0.5061 ETA: 0s - loss: 1.5515 - ac\n",
      "Epoch 00048: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 339us/sample - loss: 1.5567 - accuracy: 0.5061\n",
      "Epoch 49/100\n",
      "22848/22993 [============================>.] - ETA: 0s - loss: 1.5697 - accuracy: 0.5051\n",
      "Epoch 00049: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 347us/sample - loss: 1.5709 - accuracy: 0.5046\n",
      "Epoch 50/100\n",
      "22976/22993 [============================>.] - ETA: 0s - loss: 1.6026 - accuracy: 0.4938\n",
      "Epoch 00050: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 333us/sample - loss: 1.6027 - accuracy: 0.4938\n",
      "Epoch 51/100\n",
      "22912/22993 [============================>.] - ETA: 0s - loss: 1.6175 - accuracy: 0.4942\n",
      "Epoch 00051: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 335us/sample - loss: 1.6174 - accuracy: 0.4941\n",
      "Epoch 52/100\n",
      "22848/22993 [============================>.] - ETA: 0s - loss: 1.6181 - accuracy: 0.4903\n",
      "Epoch 00052: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 336us/sample - loss: 1.6181 - accuracy: 0.4904\n",
      "Epoch 53/100\n",
      "22816/22993 [============================>.] - ETA: 0s - loss: 1.6323 - accuracy: 0.4868\n",
      "Epoch 00053: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 336us/sample - loss: 1.6355 - accuracy: 0.4857\n",
      "Epoch 54/100\n",
      "22976/22993 [============================>.] - ETA: 0s - loss: 1.6301 - accuracy: 0.4877\n",
      "Epoch 00054: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 337us/sample - loss: 1.6298 - accuracy: 0.4877\n",
      "Epoch 55/100\n",
      "22976/22993 [============================>.] - ETA: 0s - loss: 1.6263 - accuracy: 0.4852\n",
      "Epoch 00055: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 337us/sample - loss: 1.6264 - accuracy: 0.4851\n",
      "Epoch 56/100\n",
      "22976/22993 [============================>.] - ETA: 0s - loss: 1.5970 - accuracy: 0.4952\n",
      "Epoch 00056: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 334us/sample - loss: 1.5966 - accuracy: 0.4953\n",
      "Epoch 57/100\n",
      "22880/22993 [============================>.] - ETA: 0s - loss: 1.5768 - accuracy: 0.4995\n",
      "Epoch 00057: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 342us/sample - loss: 1.5767 - accuracy: 0.4995\n",
      "Epoch 58/100\n",
      "22848/22993 [============================>.] - ETA: 0s - loss: 1.6116 - accuracy: 0.4927 ETA: 0s - loss: 1.6096 \n",
      "Epoch 00058: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 343us/sample - loss: 1.6128 - accuracy: 0.4925\n",
      "Epoch 59/100\n",
      "22880/22993 [============================>.] - ETA: 0s - loss: 1.6088 - accuracy: 0.4938\n",
      "Epoch 00059: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 338us/sample - loss: 1.6086 - accuracy: 0.4938\n",
      "Epoch 60/100\n",
      "22944/22993 [============================>.] - ETA: 0s - loss: 1.6116 - accuracy: 0.4888\n",
      "Epoch 00060: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 337us/sample - loss: 1.6116 - accuracy: 0.4889\n",
      "Epoch 61/100\n",
      "22976/22993 [============================>.] - ETA: 0s - loss: 1.6089 - accuracy: 0.4908\n",
      "Epoch 00061: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 336us/sample - loss: 1.6088 - accuracy: 0.4908\n",
      "Epoch 62/100\n",
      "22880/22993 [============================>.] - ETA: 0s - loss: 1.6177 - accuracy: 0.4917\n",
      "Epoch 00062: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 339us/sample - loss: 1.6183 - accuracy: 0.4912\n",
      "Epoch 63/100\n",
      "22880/22993 [============================>.] - ETA: 0s - loss: 1.6117 - accuracy: 0.4894\n",
      "Epoch 00063: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 337us/sample - loss: 1.6129 - accuracy: 0.4894\n",
      "Epoch 64/100\n",
      "22880/22993 [============================>.] - ETA: 0s - loss: 1.6458 - accuracy: 0.4830\n",
      "Epoch 00064: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 338us/sample - loss: 1.6468 - accuracy: 0.4828\n",
      "Epoch 65/100\n",
      "22880/22993 [============================>.] - ETA: 0s - loss: 1.6265 - accuracy: 0.4864\n",
      "Epoch 00065: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 336us/sample - loss: 1.6288 - accuracy: 0.4860\n",
      "Epoch 66/100\n",
      "22880/22993 [============================>.] - ETA: 0s - loss: 1.6244 - accuracy: 0.4873\n",
      "Epoch 00066: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 351us/sample - loss: 1.6253 - accuracy: 0.4870\n",
      "Epoch 67/100\n",
      "22912/22993 [============================>.] - ETA: 0s - loss: 1.6232 - accuracy: 0.4889\n",
      "Epoch 00067: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 338us/sample - loss: 1.6232 - accuracy: 0.4889\n",
      "Epoch 68/100\n",
      "22880/22993 [============================>.] - ETA: 0s - loss: 1.6184 - accuracy: 0.4905\n",
      "Epoch 00068: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 331us/sample - loss: 1.6168 - accuracy: 0.4910\n",
      "Epoch 69/100\n",
      "22848/22993 [============================>.] - ETA: 0s - loss: 1.6379 - accuracy: 0.4852\n",
      "Epoch 00069: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 338us/sample - loss: 1.6372 - accuracy: 0.4854\n",
      "Epoch 70/100\n",
      "22912/22993 [============================>.] - ETA: 0s - loss: 1.6276 - accuracy: 0.4905\n",
      "Epoch 00070: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 337us/sample - loss: 1.6292 - accuracy: 0.4899\n",
      "Epoch 71/100\n",
      "22944/22993 [============================>.] - ETA: 0s - loss: 1.6262 - accuracy: 0.4908\n",
      "Epoch 00071: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 345us/sample - loss: 1.6262 - accuracy: 0.4906\n",
      "Epoch 72/100\n",
      "22848/22993 [============================>.] - ETA: 0s - loss: 1.6218 - accuracy: 0.4929\n",
      "Epoch 00072: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 333us/sample - loss: 1.6223 - accuracy: 0.4927\n",
      "Epoch 73/100\n",
      "22976/22993 [============================>.] - ETA: 0s - loss: 1.6108 - accuracy: 0.4956\n",
      "Epoch 00073: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 337us/sample - loss: 1.6112 - accuracy: 0.4955\n",
      "Epoch 74/100\n",
      "22976/22993 [============================>.] - ETA: 0s - loss: 1.6303 - accuracy: 0.4927\n",
      "Epoch 00074: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 343us/sample - loss: 1.6304 - accuracy: 0.4926\n",
      "Epoch 75/100\n",
      "22976/22993 [============================>.] - ETA: 0s - loss: 1.6046 - accuracy: 0.4966\n",
      "Epoch 00075: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 339us/sample - loss: 1.6041 - accuracy: 0.4967\n",
      "Epoch 76/100\n",
      "22880/22993 [============================>.] - ETA: 0s - loss: 1.6584 - accuracy: 0.4824\n",
      "Epoch 00076: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 338us/sample - loss: 1.6585 - accuracy: 0.4828\n",
      "Epoch 77/100\n",
      "22976/22993 [============================>.] - ETA: 0s - loss: 1.6651 - accuracy: 0.4830\n",
      "Epoch 00077: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 338us/sample - loss: 1.6655 - accuracy: 0.4829\n",
      "Epoch 78/100\n",
      "22816/22993 [============================>.] - ETA: 0s - loss: 1.6561 - accuracy: 0.4846\n",
      "Epoch 00078: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 336us/sample - loss: 1.6571 - accuracy: 0.4843\n",
      "Epoch 79/100\n",
      "22880/22993 [============================>.] - ETA: 0s - loss: 1.6422 - accuracy: 0.4867\n",
      "Epoch 00079: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 334us/sample - loss: 1.6426 - accuracy: 0.4868\n",
      "Epoch 80/100\n",
      "22848/22993 [============================>.] - ETA: 0s - loss: 1.6235 - accuracy: 0.4915\n",
      "Epoch 00080: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 337us/sample - loss: 1.6240 - accuracy: 0.4914\n",
      "Epoch 81/100\n",
      "22944/22993 [============================>.] - ETA: 0s - loss: 1.6275 - accuracy: 0.4894\n",
      "Epoch 00081: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 338us/sample - loss: 1.6277 - accuracy: 0.4894\n",
      "Epoch 82/100\n",
      "22976/22993 [============================>.] - ETA: 0s - loss: 1.6332 - accuracy: 0.4906\n",
      "Epoch 00082: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 339us/sample - loss: 1.6335 - accuracy: 0.4906\n",
      "Epoch 83/100\n",
      "22976/22993 [============================>.] - ETA: 0s - loss: 1.6439 - accuracy: 0.4825\n",
      "Epoch 00083: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 343us/sample - loss: 1.6441 - accuracy: 0.4826\n",
      "Epoch 84/100\n",
      "22944/22993 [============================>.] - ETA: 0s - loss: 1.6306 - accuracy: 0.4851\n",
      "Epoch 00084: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 334us/sample - loss: 1.6312 - accuracy: 0.4851\n",
      "Epoch 85/100\n",
      "22976/22993 [============================>.] - ETA: 0s - loss: 1.6574 - accuracy: 0.4833\n",
      "Epoch 00085: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 336us/sample - loss: 1.6576 - accuracy: 0.4834\n",
      "Epoch 86/100\n",
      "22912/22993 [============================>.] - ETA: 0s - loss: 1.6392 - accuracy: 0.4866\n",
      "Epoch 00086: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 337us/sample - loss: 1.6392 - accuracy: 0.4865\n",
      "Epoch 87/100\n",
      "22816/22993 [============================>.] - ETA: 0s - loss: 1.6436 - accuracy: 0.4860\n",
      "Epoch 00087: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 338us/sample - loss: 1.6431 - accuracy: 0.4865\n",
      "Epoch 88/100\n",
      "22912/22993 [============================>.] - ETA: 0s - loss: 1.6543 - accuracy: 0.4835 ETA: 0s - los\n",
      "Epoch 00088: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 339us/sample - loss: 1.6539 - accuracy: 0.4836\n",
      "Epoch 89/100\n",
      "22976/22993 [============================>.] - ETA: 0s - loss: 1.6491 - accuracy: 0.4842\n",
      "Epoch 00089: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 339us/sample - loss: 1.6491 - accuracy: 0.4841\n",
      "Epoch 90/100\n",
      "22880/22993 [============================>.] - ETA: 0s - loss: 1.6526 - accuracy: 0.4857\n",
      "Epoch 00090: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 328us/sample - loss: 1.6530 - accuracy: 0.4856\n",
      "Epoch 91/100\n",
      "22912/22993 [============================>.] - ETA: 0s - loss: 1.6443 - accuracy: 0.4856\n",
      "Epoch 00091: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 348us/sample - loss: 1.6459 - accuracy: 0.4850\n",
      "Epoch 92/100\n",
      "22880/22993 [============================>.] - ETA: 0s - loss: 1.6345 - accuracy: 0.4861\n",
      "Epoch 00092: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 337us/sample - loss: 1.6352 - accuracy: 0.4861\n",
      "Epoch 93/100\n",
      "22912/22993 [============================>.] - ETA: 0s - loss: 1.6192 - accuracy: 0.4888\n",
      "Epoch 00093: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 337us/sample - loss: 1.6186 - accuracy: 0.4890\n",
      "Epoch 94/100\n",
      "22880/22993 [============================>.] - ETA: 0s - loss: 1.6187 - accuracy: 0.4904\n",
      "Epoch 00094: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 339us/sample - loss: 1.6181 - accuracy: 0.4907\n",
      "Epoch 95/100\n",
      "22880/22993 [============================>.] - ETA: 0s - loss: 1.6284 - accuracy: 0.4889\n",
      "Epoch 00095: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 335us/sample - loss: 1.6282 - accuracy: 0.4888\n",
      "Epoch 96/100\n",
      "22848/22993 [============================>.] - ETA: 0s - loss: 1.6264 - accuracy: 0.4923\n",
      "Epoch 00096: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 338us/sample - loss: 1.6270 - accuracy: 0.4924\n",
      "Epoch 97/100\n",
      "22944/22993 [============================>.] - ETA: 0s - loss: 1.6428 - accuracy: 0.4853 ETA: 0s -\n",
      "Epoch 00097: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 337us/sample - loss: 1.6434 - accuracy: 0.4851\n",
      "Epoch 98/100\n",
      "22976/22993 [============================>.] - ETA: 0s - loss: 1.6481 - accuracy: 0.4849\n",
      "Epoch 00098: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 339us/sample - loss: 1.6481 - accuracy: 0.4849\n",
      "Epoch 99/100\n",
      "22848/22993 [============================>.] - ETA: 0s - loss: 1.6216 - accuracy: 0.4894\n",
      "Epoch 00099: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 341us/sample - loss: 1.6221 - accuracy: 0.4894\n",
      "Epoch 100/100\n",
      "22912/22993 [============================>.] - ETA: 0s - loss: 1.6288 - accuracy: 0.4881\n",
      "Epoch 00100: saving model to data/model-checkpoints/text-Generation_char_lvl/\n",
      "22993/22993 [==============================] - 8s 348us/sample - loss: 1.6292 - accuracy: 0.4880\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(np.array(input_sequences), ys, epochs=num_epochs, verbose=1, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'accuracy')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next(seed_text, reversed_word_index=None):\n",
    "\treturns = False\n",
    "\tif not reversed_word_index:\n",
    "\t\treturns = True\n",
    "\t\treversed_word_index = {value:key for key,value in chartokenizer.word_index.items()}\n",
    "\n",
    "\tfor _ in range(21):\n",
    "\t\ttoken_list = chartokenizer.texts_to_sequences([seed_text[-sequence_length:]])[0]\n",
    "\t\ttoken_list = pad_sequences([token_list], maxlen=sequence_length, padding='pre')\n",
    "\t\tpredicted = model.predict_classes(token_list, verbose=0)\n",
    "\t\toutput_word = \"\"\n",
    "\t\ttry:\n",
    "\t\t\tseed_text += reversed_word_index[predicted[0]]\n",
    "\t\texcept:\n",
    "\t\t\tpass\n",
    "\tprint(seed_text)\n",
    "\t\n",
    "\tif returns:\n",
    "\t\treturn reversed_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Come all ye maidens young and fair heart the said now t\n",
      "Laurence went to dubling and i way soul be s\n"
     ]
    }
   ],
   "source": [
    "test1 = \"Come all ye maidens young and fair\"\n",
    "test2 = \"Laurence went to dublin\"\n",
    "reversed_word_index = predict_next(test1)\n",
    "predict_next(test2,reversed_word_index)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dffce65ae2f4323183bdad384797ae357b2eec901abd57dea0f949fd5a254ff4"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('tf_gpu': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

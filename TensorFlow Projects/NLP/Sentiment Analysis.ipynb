{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "Tokenizer = tf.keras.preprocessing.text.Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise sentences \n",
    "sentences = [\"I love my dog\", \"I LOVE MY CAT!\", \"You love my dog\", \"Do you think my dog is amazing?\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you define the number of relevant words in the string sequence that are relevant. Then you pass the train data into your tokenizer and you can check what tokens are given to your data by using the `word_index` method. The module is designed to lemmatise the words and ignores punctuation and capitalisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'my': 1,\n",
       " 'love': 2,\n",
       " 'dog': 3,\n",
       " 'i': 4,\n",
       " 'you': 5,\n",
       " 'cat': 6,\n",
       " 'do': 7,\n",
       " 'think': 8,\n",
       " 'is': 9,\n",
       " 'amazing': 10}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = 100)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also see that applying these tokens to your sentences converts them into an array of integers ready to be processed. You can see that each sequence has different number of words in them and so different length arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what if your tokeniser has not assigned a value to new words that it sees in its test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 2, 1, 3], [1, 3, 1]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = [\"i really love my dog\", \"my dog loves my shoes\"]\n",
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "test_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to tackle this is by creating a token for Out Of Vocabulary words and assigning that to any new word that you see. Now our sequences are the same length as the actual sentences even though we have still lost some meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'my': 2,\n",
       " 'love': 3,\n",
       " 'dog': 4,\n",
       " 'i': 5,\n",
       " 'you': 6,\n",
       " 'cat': 7,\n",
       " 'do': 8,\n",
       " 'think': 9,\n",
       " 'is': 10,\n",
       " 'amazing': 11}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_oov = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
    "tokenizer_oov.fit_on_texts(sentences)\n",
    "word_index = tokenizer_oov.word_index\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_seq = tokenizer_oov.texts_to_sequences(test_data)\n",
    "test_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are getting close to building the neural network and processing matrices that we have created we need to think about what if your sentences are not the same length? One method is using a *\"ragged tensor\"* which we will not look at. Another method is padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_sequences = tf.keras.preprocessing.sequence.pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  4,  2,  1,  3],\n",
       "       [ 0,  0,  0,  4,  2,  1,  6],\n",
       "       [ 0,  0,  0,  5,  2,  1,  3],\n",
       "       [ 7,  5,  8,  1,  3,  9, 10]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use kwarg \n",
    "#   padding=\"post\" to pad on the left\n",
    "#   maxlen=# to pad to a specific length\n",
    "# if maxlen is smaller than longest sentences use truncating=\"post\"/\"pre\" to define cutoffs\n",
    "padded_sequences = pad_sequences(sequences) \n",
    "padded_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sarcastic news titles\n",
    "\n",
    "Im gonna use the kaggle API to download the dataset as JSON. Train our data and test... the usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " api = KaggleApi()\n",
    " api.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from kaggle the API command to download the dataset\n",
    "# kaggle datasets download -d rmisra/news-headlines-dataset-for-sarcasm-detection\n",
    "\n",
    "# Download all files of a dataset\n",
    "# Signature: dataset_download_files(dataset, path=None, force=False, quiet=True, unzip=False)\n",
    "\n",
    "api.dataset_download_files(\"rmisra/news-headlines-dataset-for-sarcasm-detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/Sarcasm_Headlines_Dataset.json\",\"r\") as file:\n",
    "    raw = file.readlines()\n",
    "    data = {\"root\":[]}\n",
    "    for line in raw:\n",
    "        data[\"root\"].append(json.loads(line))\n",
    "\n",
    "sentences = [data[\"root\"][i][\"headline\"] for i in range(len(data[\"root\"]))]\n",
    "labels = [data[\"root\"][i][\"is_sarcastic\"] for i in range(len(data[\"root\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = 20000\n",
    "\n",
    "train_sentences = sentences[:training_size]\n",
    "train_labels = labels[:training_size]\n",
    "\n",
    "test_sentences = sentences[training_size:]\n",
    "test_labels = labels[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "train_padded = pad_sequences(train_sequences, padding=\"post\")\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_sequences, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need this block to get it to work with TensorFlow 2.x\n",
    "train_padded = np.array(train_padded)\n",
    "train_labels = np.array(train_labels)\n",
    "test_padded = np.array(test_padded)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 40)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_padded.shape\n",
    "# we have 20000 sentences, longest of which must have been 40 words long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it comes to building the neural network. Sequential is a model class which carries out the model in a sequence. \n",
    "- The first item is embedding. The direction/vector representation/tone of each word will be learned epoch by epoch\n",
    "- Next you calculate the global average of these directions given their context\n",
    "- Dense is a layer of interconnected neurons and here we have used 2 layers, the first having 24 and the latter having 1 nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "max_length = 100\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1 ,activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 100) for input KerasTensor(type_spec=TensorSpec(shape=(None, 100), dtype=tf.float32, name='embedding_1_input'), name='embedding_1_input', description=\"created by layer 'embedding_1_input'\"), but it was called on an input with incompatible shape (32, 40).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 100) for input KerasTensor(type_spec=TensorSpec(shape=(None, 100), dtype=tf.float32, name='embedding_1_input'), name='embedding_1_input', description=\"created by layer 'embedding_1_input'\"), but it was called on an input with incompatible shape (32, 40).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 100) for input KerasTensor(type_spec=TensorSpec(shape=(None, 100), dtype=tf.float32, name='embedding_1_input'), name='embedding_1_input', description=\"created by layer 'embedding_1_input'\"), but it was called on an input with incompatible shape (None, 38).\n",
      "625/625 - 2s - loss: 0.6010 - accuracy: 0.6646 - val_loss: 0.4261 - val_accuracy: 0.8189\n",
      "Epoch 2/30\n",
      "625/625 - 1s - loss: 0.3312 - accuracy: 0.8651 - val_loss: 0.3466 - val_accuracy: 0.8545\n",
      "Epoch 3/30\n",
      "625/625 - 1s - loss: 0.2488 - accuracy: 0.9018 - val_loss: 0.3420 - val_accuracy: 0.8572\n",
      "Epoch 4/30\n",
      "625/625 - 1s - loss: 0.2025 - accuracy: 0.9232 - val_loss: 0.3615 - val_accuracy: 0.8484\n",
      "Epoch 5/30\n",
      "625/625 - 1s - loss: 0.1688 - accuracy: 0.9373 - val_loss: 0.3774 - val_accuracy: 0.8518\n",
      "Epoch 6/30\n",
      "625/625 - 1s - loss: 0.1437 - accuracy: 0.9490 - val_loss: 0.4119 - val_accuracy: 0.8460\n",
      "Epoch 7/30\n",
      "625/625 - 1s - loss: 0.1242 - accuracy: 0.9557 - val_loss: 0.4401 - val_accuracy: 0.8469\n",
      "Epoch 8/30\n",
      "625/625 - 1s - loss: 0.1091 - accuracy: 0.9628 - val_loss: 0.4770 - val_accuracy: 0.8438\n",
      "Epoch 9/30\n",
      "625/625 - 1s - loss: 0.0944 - accuracy: 0.9675 - val_loss: 0.5174 - val_accuracy: 0.8411\n",
      "Epoch 10/30\n",
      "625/625 - 1s - loss: 0.0832 - accuracy: 0.9729 - val_loss: 0.5652 - val_accuracy: 0.8360\n",
      "Epoch 11/30\n",
      "625/625 - 1s - loss: 0.0740 - accuracy: 0.9761 - val_loss: 0.6054 - val_accuracy: 0.8326\n",
      "Epoch 12/30\n",
      "625/625 - 1s - loss: 0.0651 - accuracy: 0.9798 - val_loss: 0.6689 - val_accuracy: 0.8307\n",
      "Epoch 13/30\n",
      "625/625 - 1s - loss: 0.0579 - accuracy: 0.9813 - val_loss: 0.7061 - val_accuracy: 0.8268\n",
      "Epoch 14/30\n",
      "625/625 - 1s - loss: 0.0516 - accuracy: 0.9840 - val_loss: 0.7540 - val_accuracy: 0.8247\n",
      "Epoch 15/30\n",
      "625/625 - 1s - loss: 0.0457 - accuracy: 0.9869 - val_loss: 0.8200 - val_accuracy: 0.8204\n",
      "Epoch 16/30\n",
      "625/625 - 1s - loss: 0.0411 - accuracy: 0.9877 - val_loss: 0.8732 - val_accuracy: 0.8162\n",
      "Epoch 17/30\n",
      "625/625 - 1s - loss: 0.0364 - accuracy: 0.9894 - val_loss: 0.9149 - val_accuracy: 0.8168\n",
      "Epoch 18/30\n",
      "625/625 - 1s - loss: 0.0316 - accuracy: 0.9906 - val_loss: 0.9684 - val_accuracy: 0.8153\n",
      "Epoch 19/30\n",
      "625/625 - 1s - loss: 0.0287 - accuracy: 0.9916 - val_loss: 1.0334 - val_accuracy: 0.8132\n",
      "Epoch 20/30\n",
      "625/625 - 1s - loss: 0.0250 - accuracy: 0.9930 - val_loss: 1.0901 - val_accuracy: 0.8123\n",
      "Epoch 21/30\n",
      "625/625 - 1s - loss: 0.0225 - accuracy: 0.9940 - val_loss: 1.1388 - val_accuracy: 0.8126\n",
      "Epoch 22/30\n",
      "625/625 - 1s - loss: 0.0198 - accuracy: 0.9949 - val_loss: 1.2011 - val_accuracy: 0.8098\n",
      "Epoch 23/30\n",
      "625/625 - 1s - loss: 0.0178 - accuracy: 0.9953 - val_loss: 1.2601 - val_accuracy: 0.8091\n",
      "Epoch 24/30\n",
      "625/625 - 1s - loss: 0.0165 - accuracy: 0.9959 - val_loss: 1.3112 - val_accuracy: 0.8080\n",
      "Epoch 25/30\n",
      "625/625 - 1s - loss: 0.0151 - accuracy: 0.9957 - val_loss: 1.3864 - val_accuracy: 0.8077\n",
      "Epoch 26/30\n",
      "625/625 - 1s - loss: 0.0145 - accuracy: 0.9956 - val_loss: 1.4136 - val_accuracy: 0.8076\n",
      "Epoch 27/30\n",
      "625/625 - 1s - loss: 0.0134 - accuracy: 0.9963 - val_loss: 1.4724 - val_accuracy: 0.8071\n",
      "Epoch 28/30\n",
      "625/625 - 1s - loss: 0.0121 - accuracy: 0.9965 - val_loss: 1.5299 - val_accuracy: 0.8064\n",
      "Epoch 29/30\n",
      "625/625 - 1s - loss: 0.0096 - accuracy: 0.9976 - val_loss: 1.6157 - val_accuracy: 0.8028\n",
      "Epoch 30/30\n",
      "625/625 - 1s - loss: 0.0086 - accuracy: 0.9979 - val_loss: 1.6378 - val_accuracy: 0.8050\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_padded, train_labels, epochs=30, validation_data=(test_padded, test_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.5522270e-01],\n",
       "       [7.4035255e-05]], dtype=float32)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sentences = [\"granny starting to fear spiders in the garden might be real\", \"game of thrones season finale showing this sunday night\"]\n",
    "\n",
    "new_sequences = tokenizer.texts_to_sequences(new_sentences)\n",
    "new_pads = pad_sequences(new_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "model.predict(new_pads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what we see is that the model predicts the first sentence to be 80% probable to be sarcastic whereas the second sentence is very unlikely to be sarcastic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ... TBC"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "438e298719159037a0291d42ce7fd0a6edbb9efe824650195d30c7789688c3ef"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('ML-Projects': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

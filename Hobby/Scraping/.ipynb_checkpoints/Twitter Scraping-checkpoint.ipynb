{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copied from:\n",
    "\n",
    "https://github.com/ZacLanghorne/FBSentimentAnalysis/blob/master/FacebookSentimentAnalysis.ipynb\n",
    "\n",
    "steps:\n",
    "\n",
    "1. _tokenising_\n",
    "\n",
    "transforms the full sentences into words by removing stop words, conjunctions, etc\n",
    "\n",
    "2. _normalising and noise reduction_\n",
    "\n",
    "this is done by lemmatising the tweets; meaning it takes the words to their root. For example, you have are, am, is --> to be or cars, car's, car --> car. \n",
    "\n",
    "- determine word density \n",
    "- build model\n",
    "- explorative analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from nltk.corpus import twitter_samples # pre scraped tweets\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = stopwords.words('english') # enlgish tokenisation\n",
    "from nltk import FreqDist # stats \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove noise\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sherv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for tokenisation\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sherv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sherv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sherv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toeknising \n",
    "nltk.download('punkt')\n",
    "# normalising and lemmatising text\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\sherv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get twitter samples \n",
    "# scraping twitter is only allowed under certain conditions\n",
    "# we train with sample data and apply to scraped data\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what is twitter samples\n",
    "*fileids*\n",
    "- to know what the strings method takes\n",
    "\n",
    "*strings*\n",
    "- gets tweets in json format\n",
    "\n",
    "*tokenized*\n",
    "- you get tokenised tweets but we want to tokenise it ourselves to practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_samples.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = twitter_samples.strings(\"positive_tweets.json\")\n",
    "negative_tweets = twitter_samples.strings(\"negative_tweets.json\")\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenise the tweets\n",
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalising and removing noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what is re (regular expression operations)\n",
    "\n",
    "1. A regular expression (or RE) specifies a set of strings that matches it. The .sub() method substitutes it with the given string.\n",
    "\n",
    "2. re contains special characters as well so if we want to search for certain characters we need to escape them with `\\`. These characters are:\n",
    "    - `.` - searches for any character except new line\n",
    "    - `\\d` - digits (0-9)\n",
    "    - `\\D` - not a digit (0-9)\n",
    "    - `w` - word character (a-z, A-Z, 0-9, _)\n",
    "    - `W` - not a word character\n",
    "    - `s` - white space\n",
    "    - `S` - not a white space \n",
    "    <br>\n",
    "    <br>\n",
    "    Anchors (invisible positions before or after characters):\n",
    "    - `b` - word boundary (\"\\bHa\", \"Ha HaHa\") would return \"**Ha** **Ha**Ha\"\n",
    "    - `B` - Not a word boundary (\"\\bHa\", \"Ha HaHa\") would return \"Ha Ha**Ha**\"\n",
    "    - `^` - Beginning of a string (\"^http\", \"http://www.blabla/http\") would return \"**http**://www.blabla/http\"\n",
    "    - `DollarSign` - end of string (\"http`DollarSign`\", \"http://www.blabla/http\") would return \"http://www.blabla/**http**\"\n",
    "    <br>\n",
    "    <br>\n",
    "    - `[]` - searches for whatever is in the bracket eg: (\"\\d[-.]\\d\", \"1.1, 2-3, 2/2\") would return \"**1.1**, **2-3**, 2/2\" <br>\n",
    "    &nbsp; `[1-5]` creates a range and will choose ONE character in that range and list <br>\n",
    "    &nbsp; `[^a-z]` using `^` within a character set searches for everything except what's included in the set eg (\"[^b]at\",  \"cat, pat, bat\") returns \"**cat**, **pat**, bat\"\n",
    "    - `|` - either or\n",
    "    - `()` - group\n",
    "    <br>\n",
    "    <br>\n",
    "    Quantifiers:\n",
    "    - `*` - 0 or more\n",
    "    - `+` - 1 or more\n",
    "    - `?` - 0 or one\n",
    "    - `{3}` - exact number eg: (\"\\d{3}\", \"123, 1234, 12, 1\") will return  \"**123**, 1234, 12, 1\"\n",
    "    - `{3,4}` - range of numbers\n",
    "    \n",
    "When you create groups, you can select them from the returned object by the re. Group 0 is always the entire match returned. eg: <br>\n",
    "```python\n",
    "url = \"https://www.google.com, https://blabla.com\"\n",
    "pattern = re.compile(r\"https?://(www\\.)?(\\w+)(\\.\\w+)\") # group 1: www. group 2: subdomain group 3:.domain\n",
    "matches = pattern.finditer(url) # or use pattern.findall(url)\n",
    "for match in matches:\n",
    "    print(match.group(1))\n",
    "```\n",
    "\n",
    "you can also refer to these groups when substituting. eg: <br>\n",
    "```python\n",
    "subbed_url = pattern.sub(r\"\\2\\3\", url) # subs groups 2 and 3\n",
    "```\n",
    "\n",
    "Finally, you can ignore case using a flag. eg <br>\n",
    "```python\n",
    "pattern = re.compile(r\"start\", re.IGNORECASE) # shorthand flag is re.I\n",
    "matches = pattern.findall(url)\n",
    "print(matches)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "    cleaned_tokens = []\n",
    "    \n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub(r\"^https?:\\/\\/.*[\\r\\n]*\",\"\", token) # removes https://<0 or more characters><0 or more new lines>\n",
    "        token = re.sub(r\"(@[A-Za-z0-9_]+)\",\"\", token) # removes mentions from the tweet\n",
    "        \n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = \"n\" # assign names as nouns\n",
    "        elif tag.startswith(\"VB\"):\n",
    "            pos = \"v\" # assign verbs as verbs\n",
    "        else:\n",
    "            pos = \"a\"\n",
    "            \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token,pos) # reduce words to their root words\n",
    "        \n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            # gets rid of the empty tweets and any punctuation\n",
    "            cleaned_tokens.append(token.lower())\n",
    "        \n",
    "        return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_cleaned_tokens_list = []\n",
    "negative_cleaned_tokens_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokens in positive_tweet_tokens:\n",
    "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words)) \n",
    "for tokens in negative_tweet_tokens:\n",
    "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns an iterator for all the words in the dictionary\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
    "all_neg_words = get_all_words(negative_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a frequency distribution to find most common words.\n",
    "\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "freq_dist_neg = FreqDist(all_neg_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the values to a dictionary for us in naive bayes classification.\n",
    "\n",
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the lists of model ready data.\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data in to train and test data.\n",
    "\n",
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                     for tweet_dict in positive_tokens_for_model] # Label the positive data.\n",
    "\n",
    "negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                     for tweet_dict in negative_tokens_for_model] # Label the negative data.\n",
    "\n",
    "dataset = positive_dataset + negative_dataset # Combine the data sets.\n",
    "\n",
    "random.shuffle(dataset) # Shuffle the data so theres no natural ordering.\n",
    "\n",
    "train_data = dataset[:int(len(dataset)*0.7)] # First 7000 entries for train.\n",
    "test_data = dataset[int(len(dataset)*0.7):] # Final 3000 for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier.train(train_data) # must creaate dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.5376666666666666\n",
      "Most Informative Features\n",
      "                   happy = True           Positi : Negati =     19.2 : 1.0\n",
      "                      hi = True           Positi : Negati =     15.8 : 1.0\n",
      "                   hello = True           Positi : Negati =     10.0 : 1.0\n",
      "                  really = True           Negati : Positi =      7.2 : 1.0\n",
      "                       â€œ = True           Negati : Positi =      6.5 : 1.0\n",
      "                     get = True           Positi : Negati =      4.9 : 1.0\n",
      "                     hey = True           Positi : Negati =      4.9 : 1.0\n",
      "                  follow = True           Positi : Negati =      4.9 : 1.0\n",
      "                    cant = True           Negati : Positi =      3.7 : 1.0\n",
      "                   sorry = True           Negati : Positi =      3.5 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Return the accuracy and the words that are most useful in determing the sentiment.\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "# Test out the model with custom tweets!\n",
    "custom_tweet = \"Test message\"\n",
    "custom_tokens = remove_noise(nltk.word_tokenize(custom_tweet))\n",
    "\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the file to scrape.\n",
    "# url = 'D:\\User\\*File_Location*' + str(1) + '.html'\n",
    "# soup = BeautifulSoup(open(url, encoding = 'utf8').read(), 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fb = requests.get(\"https://facebook.com\")\n",
    "# TODO: create a session and log in and scrape the live feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb = open(\"facebook.html\",encoding='utf8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(fb,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the messages.\n",
    "Texts = [] # Initialise the Texts vector.\n",
    "j = 0\n",
    "for div in soup.find_all('div', class_ = '_3-96 _2let'): ## Returns the message.\n",
    "    Texts.append(div.text) ## Saves output to Texts\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Texter = []\n",
    "for div in soup.find_all('div', class_ = '_3-96 _2pio _2lek _2lel'): # Returns the person that sent the text\n",
    "        Texter.append(div.text) ## Saves output to Texter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mess_date = []\n",
    "for div in soup.find_all('div', class_ = '_3-94 _2lem'): # Returns the person that sent the text\n",
    "    mess_date.append(div.text) ## Saves output to Texter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply model to facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = []\n",
    "\n",
    "for msg in Texts:\n",
    "    text_tokens.append(nltk.word_tokenize(msg)) # Tokenize the messages.\n",
    "\n",
    "txt_cleaned_tokens_list = []\n",
    "\n",
    "for tokens in text_tokens:\n",
    "    txt_cleaned_tokens_list.append(remove_noise(tokens, stop_words)) ## Clean the messages.\n",
    "\n",
    "texts_for_analysis = get_tweets_for_model(Texts)\n",
    "\n",
    "msg_clas = []\n",
    "for txts in texts_for_analysis:\n",
    "    msg_clas.append(classifier.classify(dict([token, True] for token in txts))) ## Run the messages through a calssifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "FullTexts = list(zip(Texter,Texts, mess_date, msg_clas))\n",
    "Mess_df = pd.DataFrame(FullTexts, columns = ['Texter','Texts', 'mess_date', 'msg_clas']) # Add the sentiment to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of positive and negative messages.\n",
    "\n",
    "num_pos_msg = 0\n",
    "for msg in msg_clas:\n",
    "    if msg == 'Positive':\n",
    "        num_pos_msg += 1\n",
    "\n",
    "num_neg_msg = 0\n",
    "for msg in msg_clas:\n",
    "    if msg == 'Negative':\n",
    "        num_neg_msg += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-d14550186c4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnum_pos_msg\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_pos_msg\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnum_neg_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m \u001b[1;31m# Determine percentage of positive messages.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "num_pos_msg/(num_pos_msg + num_neg_msg)*100 # Determine percentage of positive messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrape",
   "language": "python",
   "name": "scrape"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
